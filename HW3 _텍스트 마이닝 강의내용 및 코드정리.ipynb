{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201721510 김수현\n",
    "# HW3 텍스트 마이닝 강의내용 및 코드정리\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 텍스트 마이닝의 이해\n",
    "## 텍스트 마이닝이란?\n",
    "- `정형 텍스트 데이터에서 가치와 의미가 있는 정보를 찾아내는(Mining)기술`\n",
    "- 사용자는 텍스트마이닝 기술을 통해 방대한 정보 뭉치에서 의미 있는 정보를 추출해 내고, 다른 정보와의 연계성을 파악하며, 텍스트가 가진 카테고리를 찾아내는 등, 단순한 정보 검색 그 이상의 결과를 얻어냄\n",
    "- 컴퓨터가 인간이 사용하는 언어로 기술된 정보를 깊이 분석하고 그 안에 숨겨진 정보를 발굴해 내기 위해서는 대용량 언어자원과 복잡한 통계적, 규칙적 알고리즘이 적용되야함\n",
    "\n",
    "##  텍스트 마이닝 이해를 위한 기본 요구지식\n",
    "- 자연어처리, 통계학과 선형대수, 머신러닝, 딥러닝\n",
    "\n",
    "##  텍스트 마이닝 방법\n",
    "- `NLP(Natural Language Processing) 기본도구`\n",
    "  - Tokenize, stemming, lemmatize\n",
    "  - Chunking\n",
    "  - BOW, TFIDF – sparse representation\n",
    "- `머신러닝(딥러닝)`\n",
    "  -  Naïve Bayes, Logistic egression, Decision tree, SVM\n",
    "  - Embedding(Word2Vec, Doc2Vec) – dense representation\n",
    "  - RNN(LSTM), Attention, Transformer\n",
    "  \n",
    "  \n",
    "## 텍스트 마이닝 적용분야\n",
    "- ` Document classification`\n",
    "  - Sentiment analysis, classification\n",
    "- `Document generation`\n",
    "  -  Q&A, summarization, translation\n",
    "- `Keyword extraction`\n",
    "  -  tagging/annotation\n",
    "- `Topic modeling`\n",
    "  - LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation)\n",
    "  \n",
    "## 텍스트 마이닝 도구 - 파이썬\n",
    "- `NLTK` : 가장 많이 알려진 NLP 라이브러리\n",
    "- `Scikit Learn` : 머신러닝 라이브러리 , 기본적인 NLP & 다양한 텍스트 마이닝 관련 도구 지원\n",
    "- `Gensim` : Word2Vec으로 유명, sklearn과 마찬가지로 다양한 텍스트 관련 도구 지원\n",
    "- `Keras` : RNN, seq2seq 등 딥러닝(+ pytorch) 위주의 라이브러리 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 텍스트 마이닝 방법론\n",
    "\n",
    "##  텍스트 마이닝 기본도구 (NLP중심)\n",
    "- `목적: document, sentence 등을 sparse vector로 변환`\n",
    "- `Tokenize` : 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "- `Text normalization` : 최소 단위를 표준화\n",
    "- `POS-tagging` : 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "- `Chunkin` : POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "- `BOW, TFIDF` : tokenized 결과를 이용하여 문서를 vector로 표현\n",
    "\n",
    "\n",
    "## Tokenize\n",
    "- Document를 Sentence의 집합으로 분리\n",
    "- Sentence를 Word의 집합으로 분리\n",
    "- 의미 없는 문자 등을 걸러 냄\n",
    "\n",
    "- `영어 vs. 한글`\n",
    "  - 영어는 공백(space) 기준으로 비교적 쉽게 tokenize 가능\n",
    "  - 한글은 구조상 형태소(morpheme) 분석이 필요 & 복합명사, 조사, 어미 등을 분리해내는 작업이 필요 , 영어에 비해 어렵고 정확도 낮음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPunctTokenizer를 사용한 Tokenize 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "# WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에 Don't를 Don과 '와 t로 분리하였으며, 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "- `동일한 의미의 단어가 다른 형태를 갖는 것을 보완`\n",
    "  - 다른 형태의 단어들을 통일시켜 표준단어로 변환\n",
    "- `Stemming (어간 추출)`\n",
    "  - 단수 – 복수, 현재형 – 미래형 등 단어의 다양한 변형을 하나로 통일\n",
    "  - 의미가 아닌 규칙에 의한 변환\n",
    "  - 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명\n",
    "- `Lemmatization (표제어 추출)`\n",
    "  -  사전을 이용하여 단어의 원형을 추출\n",
    "  -  품사(part-of-speech)를 고려\n",
    "  -  영어의 경우, 유명한 어휘 데이터베이스인 WordNet을 이용한 WordNet lemmatizer가 많이 쓰임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (어간 추출) - Porter stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookeri\n"
     ]
    }
   ],
   "source": [
    "# Porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking')) # 결과 cook\n",
    "print(stemmer.stem('cookery')) # 결과 cookeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (어간 추출)- Lancaster stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookery\n"
     ]
    }
   ],
   "source": [
    "#  Lancaster stemmer 예제\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "stemmer = LancasterStemmer() \n",
    "print(stemmer.stem('cooking')) # 결과 cook\n",
    "print(stemmer.stem('cookery')) # 결과 cookeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizations (표제어 추출) - WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "print(lemmatizer.lemmatize('cooking')) # 결과 cooking\n",
    "print(lemmatizer.lemmatize('cooking', pos='v')) # 결과 cook\n",
    "print(lemmatizer.lemmatize('cookbooks')) # 결과 cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-tagging\n",
    "- 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를가지는 최소단위)에 대해 품사를 결정하여 할당하는 작업\n",
    "- 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "- Text-to-speech(텍스트를 읽어주는 시스템)에서도 각 단어에 대해 올바른 발음을 하기 위해 품사 태깅을 이용\n",
    "- POS Tagging은 형태소 분석으로 번역되기도 하는데, 형태소 분석은 주어진 텍스트(원시말뭉치)를 형태소 단위로 나누는 작업을 포함하므로 앞의 토큰화, 정규화 작업에 품사 태깅을 포함한 것으로 보는 것이 타당\n",
    "- `한글 형태소 분석기 도구 : Konlpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chunking\n",
    "- Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 `주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)`를 의미\n",
    "- Chunking은 주어진 텍스트에서 이와 같은 chunk를 찾는과정\n",
    "- 즉, 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정임\n",
    "- 텍스트로부터 Information Extraction(정보추출)을 하기 위한 전단계로 보거나 혹은 Information Extraction에 포함되기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개체명 인식 (Named Entity Recognition, NER)\n",
    "- 개체명(Named Entity)은 기관, 단체, 사람, 날짜 등과 같이 특정정보에 해당하는 명사구를 의미\n",
    "- 따라서 NER은 `텍스트로부터 뭔가 의미 있는 정보를 추출하기위한 방법`으로 사용\n",
    "- 예를 들어 “James is working at Disney in London”이라는 문장이있을 때, James는 PERSON, Disney는 ORGANIZATION, London은 GPE(지리적인 장소)임을 알아내는 작업\n",
    "- 관계 인식 (Relation Extraction)\n",
    "  - NER에 의해 추출된 개체명들을 대상으로 그들 간의 관계를 추출하는 작업\n",
    "  - 특정 건물이 특정 장소에 위치하는 관계와 같은 지식을 텍스트로부터 추출할 때 사용\n",
    "  - 위 NER 예에서 in을 이용해 Disney는 London에 위치한다는 관계를 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BOW (Bag of Words)\n",
    "- `BOW는 문장안에 어떤 단어가 있느냐가 중요, 단어가 어떤 위치에 있는지는 중요하지 않음 (= 순서 없다고 가정)`\n",
    "- `Vector Space Model`\n",
    "  - 문서를 bag of words 로 표현\n",
    "  - 단어가 쓰여진 순서는 무시\n",
    "  - 모든 문서에 한번 이상 나타난 단어들에 대해 유(1)/무(0) 로 문서를 표현\n",
    "- `count vector`\n",
    "  - 단어의 유/무 대신 단어가 문서에 나타난 횟수로 표현\n",
    "  - count가 weight로 작용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW 예제\n",
    "- BoW를 만드는 과정\n",
    "  1. 우선, 각 단어에 고유한 정수 인덱스를 부여합니다.\n",
    "  2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\n",
    "\n",
    "문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\n",
    "\n",
    "입력된 문서에 대해서 단어 집합(vocaburary)을 만들어 인덱스를 할당하고, BoW를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re  \n",
    "okt=Okt()  \n",
    "\n",
    "token=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")  \n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token=okt.morphs(token)  \n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤, token에다가 넣음 \n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘김 \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣어줌, 단어의 개수는 최소 1개 이상이기 때문\n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아옴\n",
    "            bow[index]=bow[index]+1\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줌 (단어의 개수를 세는 것)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow  # 물가상승률의 인덱스는 4이며, 문서1에서 물가상승률은 2번 언급되었기 때문에 인덱스 4(다섯번째 값)에 해당하는 값이 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "- `count vector의 문제점`\n",
    "  - 많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음 ex) the, a, …\n",
    "- `TF-IDF`\n",
    "  - `단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림`\n",
    "  - tf(d, t): 문서 d에 단어 t가 나타난 횟수, count vector와 동일, 로그스케일 등 다양한 변형이 있음\n",
    "  - df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수\n",
    "  - idf(t): df(t)의 역수를 바로 쓸 수도 있으나, 여러가지 이유로 로그스케일과 스무딩을 적용한 공식을 사용, log(n/(1+df(t)), n은 전체 문서 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터프레임 사용을 위해 불러옴\n",
    "from math import log # IDF 계산을 위해 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 문서를 docs에 저장\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF, IDF, 그리고 TF-IDF 값을 구하는 함수 구현\n",
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF를 구하기,(= DTM을 데이터프레임에 저장하여 출력)\n",
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]        \n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 대한 IDF 값을 구하기\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬 출력\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text Classification with BOW/TFIDF\n",
    "- `Naïve Bayes`\n",
    "- `Logistic regression`\n",
    "  - Ridge regression\n",
    "  - Lasso regression\n",
    "- `Decision tree`\n",
    "  - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "- 베이즈 정리를 이용하여 텍스트 분류를 수행\n",
    "  - 베이즈 정리 : 조건부 확률을 계산하는 방법 중 하나\n",
    "  - P(A) 가 A가 일어날 확률, P(B)가 B가 일어날 확률, P(B|A)가 A가 일어나고나서 B가 일어날 확률, P(A|B)가 B가 일어나고나서 A가 일어날 확률이라고 해보자. 이때 P(B|A)를 쉽게 구할 수 있는 상황이라면, 아래와 같은 식을 통해 P(A|B)를 구할 수 있다.\n",
    " \n",
    " $$ P(A|B)= \\frac{P(B|A)P(A)} {P(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression\n",
    "- 분류를 위한 회귀분석\n",
    "  - 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용\n",
    "  - 종속 변수가 범주형 데이터를 대상으로 하며, 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 (classification) 기법에 해당\n",
    "- 텍스트 마이닝에서의 문제\n",
    "  - 추정해야 할 계수가 vector의 크기(단어의 수)만큼 존재하므로, 과적합이 발생하기 쉽고 많은 데이터 셋이 필요\n",
    "  -  그럼에도 불구하고 잘 작동하는 편\n",
    "  - 정규화(regulation)을 이용해 과적합 해결 노력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ridge and Lasso Regression\n",
    "-  `릿지 회귀 (Ridge regression)`\n",
    "   - 목적함수에 추정할 계수(parameter)에 대한 L2 norm(규제항)을 추가하여 모형의 과적합을 방지\n",
    "- `라쏘 회귀 (Lasso regression)`\n",
    "  - L1 norm을 규제항으로 사용함으로써 0에 가까운 계수를 0으로 만들어 영향을 거의 미치지 않는 단어들을 제외\n",
    "  -  남은 단어들로 분류의 이유에 대해 설명이 가능하다는 장점이 있음\n",
    "  - feature selection의 효과가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서분류의 활용 - Sentiment Analysis (감성분석)\n",
    "- 소비자의 감성과 관련된 텍스트 정보를 자동으로 추출하는 텍스트 마이닝(Text Mining) 기술의 한 영역 \n",
    "- 문서를 작성한 사람의 감정을 추출해 내는 기술로 문서의 주제보다 어떠한 감정을 가지고 있는가를 판단하여 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 텍스트 마이닝의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "- 차원의 저주\n",
    "  - Extremely sparse data\n",
    "  - 각 데이터 간의 거리가 너무 멀게 위치\n",
    "- 해결방법\n",
    "  - 더 많은 데이터\n",
    "  - Dimension reduction\n",
    "    - feature selection\n",
    "    - feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 빈도의 불균형\n",
    "-  Zipf’s law(멱법칙)\n",
    "   - 극히 소수의 데이터가 결정적인 영향을 미치게 됨\n",
    "-  해결방안\n",
    "   - feature selection\n",
    "      - 빈도 높은 단어를 삭제\n",
    "      - 심한 경우 50% 삭제\n",
    "   - Boolean BOW 사용\n",
    "      - 1이상이면 1로 변환 (유/무)\n",
    "   - log 등의 함수를 이용해weight를 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어가 쓰인 순서정보의 손실 (BOW의 가장 큰 문제)\n",
    "- 통계에 의한 의미 파악 vs. 순서에 의한 의미 파악\n",
    "- Loss of sequence information\n",
    "  -  단어들의 순서 – context가 중요\n",
    "  -  특히 번역과 같은 sequence-to-sequence 문제에서 매우 중요\n",
    "- 해결방안\n",
    "  - n-gram\n",
    "    - 부분적 해결, 주로 classification 문제에서 유용\n",
    "  - Deep learning\n",
    "    -  RNN, Attention, Transformer, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 문제 해결을 위한 방안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (차원 축소)\n",
    "- 차원의 저주를 해결하기 위한 노력\n",
    "- `Feature selection`\n",
    "  - Manual, Regularization(Lasso)\n",
    "- `Feature extractio`n\n",
    "  - PCA, LSA(SVD)\n",
    "- `Embedding`\n",
    "  - Word embedding, Document embedding\n",
    "- `Deep Learning`\n",
    "  - RBM, Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- 머신러닝에서 컴퓨터가 스스로 학습하려면, 즉 컴퓨터가 입력받은 데이터를 분석하여 일정한 패턴이나 규칙을 찾아내려면 사람이 인지하는 데이터를 컴퓨터가 인지할 수 있는 데이터로 변환해 주어야 합니다. \n",
    "- 이때 데이터별로 어떤 특징을 가지고 있는지를 찾아내고, 그것을 토대로 데이터를 벡터로 변환하는 작업\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주성분 분석 (Principal Component Analysis)\n",
    "- 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소\n",
    "- 선형결합 (linear combination)\n",
    "   -  X (데이터, nxp): n(sample의 수), p(변수의 수)\n",
    "   - Xi: i번째 feature에 대한 크기 n의 vector\n",
    "   - Z(X의 선형결합으로 이루어진 새로운 변수, nxp) = XVT\n",
    "   - VT: 새로운 축 (pxp)  공분산행렬의 고유벡터로 구성\n",
    "- 공분산행렬\n",
    "  -  고유값이 큰 순서대로 고유벡터를 정렬하여 차원 선택\n",
    "  -  선택된 고유벡터와 X의 선형결합으로 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재의미분석 LSA(Latent Semantic Analysis)\n",
    "- 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이라고 볼 수 있음\n",
    "\n",
    "-  Ut(m, t), Σ(t, t), VtT(t, n)\n",
    "  -  m: sample의 수, n: 변수의 수, t: 축소한 잠재의미의 차원\n",
    "  - Vt: 원래 A의 공분산행렬의 고유벡터로 이루어진 행렬\n",
    "- Ut Σ (m, t)\n",
    "  -  각 sample에 대해서 n -> t로 변수가 줄어든 data set\n",
    "  -  각 문서에서 잠재의미 별 비중\n",
    "-  Σ VtT (t, n)\n",
    " - 각 변수에 대해서 sample의 차원을 t로 줄임\n",
    " - 각 단어에서 잠재의미 별 비중"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 잠재의미분석(LSA) 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4*9 크기 가지는 DTM 생성\n",
    "import numpy as np\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 풀 SVD(full SVD)를 수행 단, 여기서는 대각 행렬의 변수명을 Σ가 아니라 S를 사용 또한 V의 전치 행렬을 VT라고 함\n",
    "U, s, VT = np.linalg.svd(A, full_matrices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24  0.75  0.    0.62]\n",
      " [ 0.51  0.44 -0.   -0.74]\n",
      " [ 0.83 -0.49 -0.    0.27]\n",
      " [ 0.   -0.    1.   -0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소수점 두번째 자리까지만 출력\n",
    "# 4 × 4의 크기를 가지는 직교 행렬 U 생성\n",
    "print(U.round(2))\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대각 행렬 S\n",
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy의 linalg.svd()는 특이값 분해의 결과로 대각 행렬이 아니라 특이값의 리스트를 반환 그러므로 앞서 본 수식의 형식으로 보려면 이를 다시 대각 행렬로 바꿔줘야 함 \n",
    "S = np.zeros((4, 9)) # 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S[:4, :4] = np.diag(s) # 특이값을 대각행렬에 삽입\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.31  0.31  0.28  0.8   0.09  0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [-0.    0.35  0.35 -0.16 -0.25  0.8  -0.16  0.    0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 × 9의 크기를 가지는 직교 행렬 VT(V의 전치 행렬)가 생성\n",
    "# U × S × VT를 하면 기존의 행렬 A가 나와야 함, Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴\n",
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# t를 정하고(t=2),절단된 SVD(Truncated SVD)를 수행\n",
    "# 대각 행렬 S 내의 특이값 중에서 상위 2개만 남기고 제거\n",
    "S=S[:2,:2]\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24  0.75]\n",
      " [ 0.51  0.44]\n",
      " [ 0.83 -0.49]\n",
      " [ 0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 직교 행렬 U에 대해서도 2개의 열만 남기고 제거\n",
    "U=U[:,:2]\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.31  0.31  0.28  0.8   0.09  0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 행렬 V의 전치 행렬인 VT에 대해서 2개의 행만 남기고 제거\n",
    "VT=VT[:2,:]\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 축소된 행렬 U, S, VT에 대해서 다시 U × S × VT연산을 하면 기존의 A와는 다른 결과가 나오게 됨\n",
    "# 값이 손실되었기 때문에 이 세 개의 행렬로는 이제 기존의 A행렬을 복구할 수 없음\n",
    "#  U × S × VT연산을 해서 나오는 값을 A_prime이라 하고 기존의 행렬 A와 값을 비교\n",
    "\n",
    "A_prime=np.dot(np.dot(U,S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))\n",
    "\n",
    "# 결과\n",
    "# 대체적으로 기존에 0인 값들은 0에 가가운 값이 나오고, 1인 값들은 1에 가까운 값이 나오는 것을 볼 수 있음\n",
    "# 축소된 U는 4 × 2의 크기를 가지는데, 이는 잘 생각해보면 문서의 개수 × 토픽의 수 t의 크기임, 단어의 개수인 9는 유지되지 않는데 문서의 개수인 4의 크기가 유지되었으니 4개의 문서 각각을 2개의 값으로 표현하고 있음 즉, U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터라고 볼 수 있다. \n",
    "# 축소된 VT는 2 × 9의 크기를 가지는데, 이는 잘 생각해보면 토픽의 수 t × 단어의 개수의 크기임, VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터라고 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재의미분석의 활용\n",
    "- 문서 간의 유사도\n",
    "  -  Count vector나 TFIDF에 cosine similarity를 직접 적용하는경우, 물리적인 단어로만 유사도를 측정하게 됨\n",
    "  - 잠재의미분석을 활용하면 직접적인 단어가 아니라 의미적으로 유사한(문서에서 함께 많이 등장한) 단어들로 유사도를 측정하는 것이 가능할 것으로 기대\n",
    "- 단어 간의 유사도\n",
    "  -  마찬가지로 주어진 문서 집합에서 단어들이 어떤 유사도를 가지는 지 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특이값 분해 SVD (singular vector decomposition)\n",
    "- SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말함\n",
    "- A=UΣVT\n",
    "- 여기서 각 3개의 행렬은 다음과 같은 조건을 만족함 \n",
    "- U:m×m 직교행렬 (AAT=U(ΣΣT)UT)\n",
    "- V:n×n 직교행렬 (ATA=V(ΣTΣ)VT)\n",
    "- Σ:m×n 직사각 대각행렬\n",
    "- 여기서 직교행렬(orthogonal matrix)이란 자신과 자신의 전치 행렬(transposed matrix)의 곱 또는 이를 반대로 곱한 결과가 단위행렬(identity matrix)이 되는 행렬을 말함또한 대각행렬(diagonal matrix)이란 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 의미함\n",
    "- 이때 SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값(singular value)라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- `기계 학습 및 자연어 처리 분야에서 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법`\n",
    "\n",
    "- 토픽은 주제를 의미하는 용어로 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 주제로부터 특정 단어들이 파생되어 나올 확률분포가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산\n",
    "- θ: 문서들이 각 주제들에 속할 확률분포\n",
    "  -  디리클레분포의 매개변수인 <알파>에 의해 결정\n",
    "- N: 특정 문서에 속한 단어의 집합\n",
    "- M: 전체 문서의 집합\n",
    "- z: 문서 내의 단어들이 주제들에 속할 확률분포\n",
    "- θ에 의한 다항분포로 선택\n",
    "- β: 각 주제가 특정 단어를 생성할 확률을 나타내는 확률분포\n",
    "- 결국 z와 β에 의해 실제 문서들의 단어분포인 w가 결정\n",
    "- w만이 실제로 문서들을 통해 주어진 분포이고 나머지는 모두 잠재변수\n",
    "- LDA 알고리즘에서는 주어진 문서와 토픽들의 사전확률 분포인 α와 토픽 내에서 단어\n",
    "의 사전확률분포인 β의 파라미터 값을 활용해 반복적인 시뮬레이션을 통해 z와 θ를\n",
    "추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "- 단어에 대한 vector의 dimension reduction이 목표\n",
    "- 단어의 표현\n",
    "  -  Term-Document Matrix에서 Document 별 count vector\n",
    "     -  일반화가 어려움\n",
    "  -  **one-hot-encoding**: extremely sparse\n",
    "- Word embedding:\n",
    "  -  one-hot-encoding으로 표현된 단어를 **dense vector**로 변환\n",
    "  - 변환된 vector를 이용하여 학습\n",
    "  -  최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    "  - 학습목적 관점에서의 단어의 의미를 내포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding을 이용한 문서 분류\n",
    "- BOW와는 다른 관점의 문서 표현\n",
    "  -  document: 제한된 maxlen 개의 word sequence (앞이나 뒤를 잘라냄)\n",
    "  -  word: one-hot-vector에서 저차원(reduced_dim)으로 embedding된 dense vector\n",
    "  -  즉 document는 (maxlen, reduced_dim)의 2차원 행렬로 표현\n",
    "- 단순한 분류모형 (sequence 무시)\n",
    "  -  (maxlen, reduced_dim) 차원의 document를 maxlen*reduced_dim 차원으로 펼쳐서 분류모형에 적용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "- 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식 \n",
    "\n",
    "- 원-핫 인코딩을 두 가지 과정\n",
    "  1. 각 단어에 고유한 인덱스를 부여. (정수 인코딩)\n",
    "  2. 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding 예제\n",
    "\n",
    "문장: 나는 자연어 처리를 배운다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "# 코엔엘파이의 Okt 형태소 분석기를 통해서 우선 문장에 대해서 토큰화를 수행\n",
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# 각 토큰에 대해서 고유한 인덱스(index)를 부여\n",
    "word2index={}\n",
    "for voca in token:\n",
    "     if voca not in word2index.keys():\n",
    "       word2index[voca]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰을 입력하면 해당 토큰에 대한 one-hot 벡터를 만들어내는 함수 만듦\n",
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index))\n",
    "       index=word2index[word]\n",
    "       one_hot_vector[index]=1\n",
    "       return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\",word2index) # 해당 함수에 '자연어'라는 토큰을 입력으로 넣어봤더니 [0, 0, 1, 0, 0, 0]라는 벡터가 나옴 자연어는 단어 집합에서 인덱스가 2이므로, 자연어를 표현하는 one-hot 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "-  문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "  - CBOW: 주변단어들을 이용해 다음 단어를 예측\n",
    "  -  Skip-gram: 한 단어의 주변단어들을 예측\n",
    "  \n",
    "- 단어의 위치에 기반하여 의미를 내포하는 vector 생성\n",
    "- 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨\n",
    "- 단어 간의 유사성을 이용하여 연산이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo (Embeddings from Language Model)\n",
    "- 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론\n",
    "- 이전의 대표적인 임베딩 기법인 Word2Vec이나 GloVe 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는것을 반영하지 못하는 것에 비해, ELMo는 이러한 문맥을반영하기 위해 개발된 워드 임베딩 기법\n",
    "- 문맥의 파악을 위해 biLSTM으로 학습된 모형을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "- 텍스트 마이닝에서의 전이학습\n",
    "  - feature level: 단어에 대한 dense vector(word embedding)를 새로 학습하지 않고 학습된 vector를 그대로 가져다 씀\n",
    "  - model level: word embedding과 모형 전체를 가져다 학습\n",
    "  - Word2Vec, Glove, ELMo 등의 사전학습된 word embedding이 전이학습에 많이 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding\n",
    "- Word2Vec은 word에 대해 dense vector를 생성하지만, document vector는 여전히 sparse\n",
    "- Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 densevector를 생성\n",
    "- 이 dense vector를 이용해 매칭, 분류 등의 작업 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM (Restricted Boltzmann Machine)\n",
    "- 사전학습 목적으로 개발\n",
    "  -  G. Hinton에 의해 제안\n",
    "  -  차원을 변경하면서 원래의 정보량 유지가 목적\n",
    "- 정보량을 물리학의 에너지 함수로 표현\n",
    "-  Deep NN의 vanishing gradient 문제 해결을 위해 제안\n",
    "  -  batch normalization, Dropout, ReLU 등의 기법으로 인해 문제가 해결되면서 지금은 많이 쓰이지 않음\n",
    "- 사전학습을 통한 차원 축소에 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "-  RBM과 유사한 개념\n",
    "  -  encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "- 작동방식은 PCA와 유사\n",
    "- 데이터에 내재된 일정한 구조 – 연관성을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context(sequence)의 파악\n",
    "- N-gram\n",
    "  -  문맥(context)를 파악하기 위한 전통적 방법\n",
    "  - bi-gram, tri-gram, …\n",
    "  - 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리\n",
    "- 딥러닝 – RNN\n",
    "  -  문장을 단어들의 sequence 혹은 series로 처리\n",
    "  -  뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    "  -  그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)\n",
    "- RNN의 문제\n",
    "  - 문장이 길수록 층이 깊은 형태를 갖게 됨 \n",
    "  - 경사가 소실되는 문제 발생 \n",
    "  - 앞부분의 단어 정보가 학습되지 않음\n",
    "- LSTM: 직통 통로를 만들어 RNN의 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 신경망(Convolutional Neural Networks,CNN)\n",
    "- 합성곱층과 풀링층이 번갈아가면서 이미지의 특징 단계적으로 추출, 마지막에 분류기를 통해 이미지 판별하는 구조\n",
    "- CNN은 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음\n",
    "- 그러나 CNN이 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥을 학습하여 문서를 분류하는 연구가 처음있었으며, 의외로 뛰어난 성능을 보이게 되면서 자연어 처리에서의 활용분야가 넓어지게 됨\n",
    "- CNN은 합성곱층(conolution layer)와 풀링층(pooling)으로 구성되며, 합성곱층은 2차원 이미지에서 특정 영역의 특징을 추출하는 역할을 하는데, 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence\n",
    "- 지금까지는 입력은 sequence, 출력은 하나의 값인 경우가 일반적이었으나, 번역, chat-bot, summarize등은 출력도 sequence가 되어야 함\n",
    "- encoder, decoder의 구조를 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "- 입력의 단어들로부터 출력 단어에 직접 링크를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (Self-attention)\n",
    "- 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "  - 즉 입력 -> 출력으로의 attention 외에 입력 단어들 간의 attention, 입력 + 출력 -> 출력으로의 attention을 추가\n",
    "- encoder와 decoder가 서로 다른 attention 구조를 사용\n",
    "- multi-head attention vs masked multi-head attention\n",
    "- RNN이 사라지고 self-attention이 이를 대신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations formTransformer)\n",
    "- 양방향 transformer 인코더를 사용\n",
    "  -  transformer에 기반한 OpenAI GPT와의 차이\n",
    "- transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택\n",
    "- 거의 모든 분야에서 top score를 기록\n",
    "- segment, position embedding을 사용\n",
    "- 다양한 text mining task에 전이학습을 이용해 적용 가능한 구조를 제안"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
