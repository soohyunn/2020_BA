{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201721510 김수현\n",
    "# 8주차 텍스트마이닝 실습\n",
    "# 01. featureset - bagofwords, tfidf, countvector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features of Text\n",
    "**Text classification**: a way to categorize documents or pieces of text. By examining the word usage in a piece of text, classifiers can decide what class label to assign to it.\n",
    "<br>A binary classifier decides between two labels, such as positive or negative.\n",
    "<br>**Multi-label classifier** can assign one or more labels to a piece of text.\n",
    "<br>Classification works by learning from **labeled feature sets**.\n",
    "<br>A **feature set** is basically a key-value mapping of feature names to feature values. \n",
    "<br>In the case of text classification, the feature names are usually words, and the values are all True. \n",
    "<br>As the documents may have unknown words, and the number of possible words may be very large, words that don't occur in the text are omitted, instead of including them in a feature set with the value False.\n",
    "\n",
    "## Bag of words feature extraction\n",
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. \n",
    "<br>The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict. \n",
    "<br>The **bag of words** model is the simplest method; it constructs a word presence feature set from all the words of an instance. \n",
    "<br>This method doesn't care about the order of the words, or how many times a word occurs, all that\n",
    "matters is whether the word is present in a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 문서(word tokenize된 결과)에 대해 feature set을 dictionary 형태로 구성해서 반환\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words]) #있는 단어들에 대해 True로 표시, 없는 단어는 표시 안 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': True, 'quick': True, 'brown': True, 'fox': True}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(['the', 'quick', 'brown', 'fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Corpus\n",
    "https://www.nltk.org/book/ch02.html\n",
    "\n",
    "\n",
    "### movie_reviews\n",
    "NLTK Corpus 중 movie review 수집 데이터로, 감성분석 결과(pos, neg)가 label로 붙어 있음<br>\n",
    "Compiler: Pang, Lee<br>\n",
    "Contents: 2k movie reviews with sentiment polarity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\soo\n",
      "[nltk_data]     hyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review count: 2000\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "['neg', 'pos']\n",
      "\"neg\" reviews: 1000\n",
      "\"pos\" reviews: 1000\n",
      "id: neg/cv000_29416.txt\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
      "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\soo\n",
      "[nltk_data]     hyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print(movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print(movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('\"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('\"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('id:', fileid)\n",
    "print(movie_reviews.raw(fileid)[:500]) #첫번째 문서의 내용을 500자까지만 출력\n",
    "print(movie_reviews.sents(fileid)[:2]) #첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
    "print(movie_reviews.words(fileid)[:10]) #첫번째 문서를 word tokenize한 결과 중 앞 열 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bag of words model을 이용한 feature 추출 첫번째 방법**\n",
    "\n",
    "label 별로 각 문서들에 대한 feature를 생성 - bag_of_words() 함수를 이용\n",
    "\n",
    "collections(https://docs.python.org/2/library/collections.html) - High-performance container datatypes - 를 이용하여 feature set을 저장. 하나의 label(ex. 'neg')에 1,000개의 feature set을 할당해야 하므로 단순 dictionary가 아닌 container를 사용. 각 feature는 dictionary로 이루어져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['neg', 'pos'])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "#feature 추출을 위한 함수로, 앞서 정의한 bag_of_words()를 사용\n",
    "def label_feats_from_corpus(corp, feature_detector=bag_of_words): \n",
    "    label_feats = collections.defaultdict(list) # container 초기화\n",
    "    for label in corp.categories(): #''neg', 'pos' 각 label에 대해\n",
    "        for fileid in corp.fileids(categories=[label]): #각 label에 해당하는 문서들에 대해\n",
    "            feats = feature_detector(corp.words(fileids=[fileid])) #주어진 문서를 bag_of_words feature로 변환\n",
    "            label_feats[label].append(feats) #container에 feature 추가\n",
    "    return label_feats\n",
    "\n",
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "print(lfeats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lfeats['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot, :, two, teen, couples, go, to, a, church, party, ,, drink, and, then, drive, ., they, get, into, an, accident, one, of, the, guys, dies, but, his, girlfriend, continues, see, him, in, her, life, has, nightmares, what, \\', s, deal, ?, watch, movie, \", sorta, find, out, critique, mind, -, fuck, for, generation, that, touches, on, very, cool, idea, presents, it, bad, package, which, is, makes, this, review, even, harder, write, since, i, generally, applaud, films, attempt, break, mold, mess, with, your, head, such, (, lost, highway, &, memento, ), there, are, good, ways, making, all, types, these, folks, just, didn, t, snag, correctly, seem, have, taken, pretty, neat, concept, executed, terribly, so, problems, well, its, main, problem, simply, too, jumbled, starts, off, normal, downshifts, fantasy, world, you, as, audience, member, no, going, dreams, characters, coming, back, from, dead, others, who, look, like, strange, apparitions, disappearances, looooot, chase, scenes, tons, weird, things, happen, most, not, explained, now, personally, don, trying, unravel, film, every, when, does, give, me, same, clue, over, again, kind, fed, up, after, while, biggest, obviously, got, big, secret, hide, seems, want, completely, until, final, five, minutes, do, make, entertaining, thrilling, or, engaging, meantime, really, sad, part, arrow, both, dig, flicks, we, actually, figured, by, half, way, point, strangeness, did, start, little, bit, sense, still, more, guess, bottom, line, movies, should, always, sure, before, given, password, enter, understanding, mean, showing, melissa, sagemiller, running, away, visions, about, 20, throughout, plain, lazy, !, okay, people, chasing, know, need, how, giving, us, different, offering, further, insight, down, apparently, studio, took, director, chopped, themselves, shows, might, ve, been, decent, here, somewhere, suits, decided, turning, music, video, edge, would, actors, although, wes, bentley, seemed, be, playing, exact, character, he, american, beauty, only, new, neighborhood, my, kudos, holds, own, entire, feeling, unraveling, overall, doesn, stick, because, entertain, confusing, rarely, excites, feels, redundant, runtime, despite, ending, explanation, craziness, came, oh, horror, slasher, flick, packaged, someone, assuming, genre, hot, kids, also, wrapped, production, years, ago, sitting, shelves, ever, whatever, skip, where, joblo, nightmare, elm, street, 3, 7, /, 10, blair, witch, 2, crow, 9, salvation, 4, stir, echoes, 8'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(lfeats['neg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plot': True,\n",
       " ':': True,\n",
       " 'two': True,\n",
       " 'teen': True,\n",
       " 'couples': True,\n",
       " 'go': True,\n",
       " 'to': True,\n",
       " 'a': True,\n",
       " 'church': True,\n",
       " 'party': True,\n",
       " ',': True,\n",
       " 'drink': True,\n",
       " 'and': True,\n",
       " 'then': True,\n",
       " 'drive': True,\n",
       " '.': True,\n",
       " 'they': True,\n",
       " 'get': True,\n",
       " 'into': True,\n",
       " 'an': True,\n",
       " 'accident': True,\n",
       " 'one': True,\n",
       " 'of': True,\n",
       " 'the': True,\n",
       " 'guys': True,\n",
       " 'dies': True,\n",
       " 'but': True,\n",
       " 'his': True,\n",
       " 'girlfriend': True,\n",
       " 'continues': True,\n",
       " 'see': True,\n",
       " 'him': True,\n",
       " 'in': True,\n",
       " 'her': True,\n",
       " 'life': True,\n",
       " 'has': True,\n",
       " 'nightmares': True,\n",
       " 'what': True,\n",
       " \"'\": True,\n",
       " 's': True,\n",
       " 'deal': True,\n",
       " '?': True,\n",
       " 'watch': True,\n",
       " 'movie': True,\n",
       " '\"': True,\n",
       " 'sorta': True,\n",
       " 'find': True,\n",
       " 'out': True,\n",
       " 'critique': True,\n",
       " 'mind': True,\n",
       " '-': True,\n",
       " 'fuck': True,\n",
       " 'for': True,\n",
       " 'generation': True,\n",
       " 'that': True,\n",
       " 'touches': True,\n",
       " 'on': True,\n",
       " 'very': True,\n",
       " 'cool': True,\n",
       " 'idea': True,\n",
       " 'presents': True,\n",
       " 'it': True,\n",
       " 'bad': True,\n",
       " 'package': True,\n",
       " 'which': True,\n",
       " 'is': True,\n",
       " 'makes': True,\n",
       " 'this': True,\n",
       " 'review': True,\n",
       " 'even': True,\n",
       " 'harder': True,\n",
       " 'write': True,\n",
       " 'since': True,\n",
       " 'i': True,\n",
       " 'generally': True,\n",
       " 'applaud': True,\n",
       " 'films': True,\n",
       " 'attempt': True,\n",
       " 'break': True,\n",
       " 'mold': True,\n",
       " 'mess': True,\n",
       " 'with': True,\n",
       " 'your': True,\n",
       " 'head': True,\n",
       " 'such': True,\n",
       " '(': True,\n",
       " 'lost': True,\n",
       " 'highway': True,\n",
       " '&': True,\n",
       " 'memento': True,\n",
       " ')': True,\n",
       " 'there': True,\n",
       " 'are': True,\n",
       " 'good': True,\n",
       " 'ways': True,\n",
       " 'making': True,\n",
       " 'all': True,\n",
       " 'types': True,\n",
       " 'these': True,\n",
       " 'folks': True,\n",
       " 'just': True,\n",
       " 'didn': True,\n",
       " 't': True,\n",
       " 'snag': True,\n",
       " 'correctly': True,\n",
       " 'seem': True,\n",
       " 'have': True,\n",
       " 'taken': True,\n",
       " 'pretty': True,\n",
       " 'neat': True,\n",
       " 'concept': True,\n",
       " 'executed': True,\n",
       " 'terribly': True,\n",
       " 'so': True,\n",
       " 'problems': True,\n",
       " 'well': True,\n",
       " 'its': True,\n",
       " 'main': True,\n",
       " 'problem': True,\n",
       " 'simply': True,\n",
       " 'too': True,\n",
       " 'jumbled': True,\n",
       " 'starts': True,\n",
       " 'off': True,\n",
       " 'normal': True,\n",
       " 'downshifts': True,\n",
       " 'fantasy': True,\n",
       " 'world': True,\n",
       " 'you': True,\n",
       " 'as': True,\n",
       " 'audience': True,\n",
       " 'member': True,\n",
       " 'no': True,\n",
       " 'going': True,\n",
       " 'dreams': True,\n",
       " 'characters': True,\n",
       " 'coming': True,\n",
       " 'back': True,\n",
       " 'from': True,\n",
       " 'dead': True,\n",
       " 'others': True,\n",
       " 'who': True,\n",
       " 'look': True,\n",
       " 'like': True,\n",
       " 'strange': True,\n",
       " 'apparitions': True,\n",
       " 'disappearances': True,\n",
       " 'looooot': True,\n",
       " 'chase': True,\n",
       " 'scenes': True,\n",
       " 'tons': True,\n",
       " 'weird': True,\n",
       " 'things': True,\n",
       " 'happen': True,\n",
       " 'most': True,\n",
       " 'not': True,\n",
       " 'explained': True,\n",
       " 'now': True,\n",
       " 'personally': True,\n",
       " 'don': True,\n",
       " 'trying': True,\n",
       " 'unravel': True,\n",
       " 'film': True,\n",
       " 'every': True,\n",
       " 'when': True,\n",
       " 'does': True,\n",
       " 'give': True,\n",
       " 'me': True,\n",
       " 'same': True,\n",
       " 'clue': True,\n",
       " 'over': True,\n",
       " 'again': True,\n",
       " 'kind': True,\n",
       " 'fed': True,\n",
       " 'up': True,\n",
       " 'after': True,\n",
       " 'while': True,\n",
       " 'biggest': True,\n",
       " 'obviously': True,\n",
       " 'got': True,\n",
       " 'big': True,\n",
       " 'secret': True,\n",
       " 'hide': True,\n",
       " 'seems': True,\n",
       " 'want': True,\n",
       " 'completely': True,\n",
       " 'until': True,\n",
       " 'final': True,\n",
       " 'five': True,\n",
       " 'minutes': True,\n",
       " 'do': True,\n",
       " 'make': True,\n",
       " 'entertaining': True,\n",
       " 'thrilling': True,\n",
       " 'or': True,\n",
       " 'engaging': True,\n",
       " 'meantime': True,\n",
       " 'really': True,\n",
       " 'sad': True,\n",
       " 'part': True,\n",
       " 'arrow': True,\n",
       " 'both': True,\n",
       " 'dig': True,\n",
       " 'flicks': True,\n",
       " 'we': True,\n",
       " 'actually': True,\n",
       " 'figured': True,\n",
       " 'by': True,\n",
       " 'half': True,\n",
       " 'way': True,\n",
       " 'point': True,\n",
       " 'strangeness': True,\n",
       " 'did': True,\n",
       " 'start': True,\n",
       " 'little': True,\n",
       " 'bit': True,\n",
       " 'sense': True,\n",
       " 'still': True,\n",
       " 'more': True,\n",
       " 'guess': True,\n",
       " 'bottom': True,\n",
       " 'line': True,\n",
       " 'movies': True,\n",
       " 'should': True,\n",
       " 'always': True,\n",
       " 'sure': True,\n",
       " 'before': True,\n",
       " 'given': True,\n",
       " 'password': True,\n",
       " 'enter': True,\n",
       " 'understanding': True,\n",
       " 'mean': True,\n",
       " 'showing': True,\n",
       " 'melissa': True,\n",
       " 'sagemiller': True,\n",
       " 'running': True,\n",
       " 'away': True,\n",
       " 'visions': True,\n",
       " 'about': True,\n",
       " '20': True,\n",
       " 'throughout': True,\n",
       " 'plain': True,\n",
       " 'lazy': True,\n",
       " '!': True,\n",
       " 'okay': True,\n",
       " 'people': True,\n",
       " 'chasing': True,\n",
       " 'know': True,\n",
       " 'need': True,\n",
       " 'how': True,\n",
       " 'giving': True,\n",
       " 'us': True,\n",
       " 'different': True,\n",
       " 'offering': True,\n",
       " 'further': True,\n",
       " 'insight': True,\n",
       " 'down': True,\n",
       " 'apparently': True,\n",
       " 'studio': True,\n",
       " 'took': True,\n",
       " 'director': True,\n",
       " 'chopped': True,\n",
       " 'themselves': True,\n",
       " 'shows': True,\n",
       " 'might': True,\n",
       " 've': True,\n",
       " 'been': True,\n",
       " 'decent': True,\n",
       " 'here': True,\n",
       " 'somewhere': True,\n",
       " 'suits': True,\n",
       " 'decided': True,\n",
       " 'turning': True,\n",
       " 'music': True,\n",
       " 'video': True,\n",
       " 'edge': True,\n",
       " 'would': True,\n",
       " 'actors': True,\n",
       " 'although': True,\n",
       " 'wes': True,\n",
       " 'bentley': True,\n",
       " 'seemed': True,\n",
       " 'be': True,\n",
       " 'playing': True,\n",
       " 'exact': True,\n",
       " 'character': True,\n",
       " 'he': True,\n",
       " 'american': True,\n",
       " 'beauty': True,\n",
       " 'only': True,\n",
       " 'new': True,\n",
       " 'neighborhood': True,\n",
       " 'my': True,\n",
       " 'kudos': True,\n",
       " 'holds': True,\n",
       " 'own': True,\n",
       " 'entire': True,\n",
       " 'feeling': True,\n",
       " 'unraveling': True,\n",
       " 'overall': True,\n",
       " 'doesn': True,\n",
       " 'stick': True,\n",
       " 'because': True,\n",
       " 'entertain': True,\n",
       " 'confusing': True,\n",
       " 'rarely': True,\n",
       " 'excites': True,\n",
       " 'feels': True,\n",
       " 'redundant': True,\n",
       " 'runtime': True,\n",
       " 'despite': True,\n",
       " 'ending': True,\n",
       " 'explanation': True,\n",
       " 'craziness': True,\n",
       " 'came': True,\n",
       " 'oh': True,\n",
       " 'horror': True,\n",
       " 'slasher': True,\n",
       " 'flick': True,\n",
       " 'packaged': True,\n",
       " 'someone': True,\n",
       " 'assuming': True,\n",
       " 'genre': True,\n",
       " 'hot': True,\n",
       " 'kids': True,\n",
       " 'also': True,\n",
       " 'wrapped': True,\n",
       " 'production': True,\n",
       " 'years': True,\n",
       " 'ago': True,\n",
       " 'sitting': True,\n",
       " 'shelves': True,\n",
       " 'ever': True,\n",
       " 'whatever': True,\n",
       " 'skip': True,\n",
       " 'where': True,\n",
       " 'joblo': True,\n",
       " 'nightmare': True,\n",
       " 'elm': True,\n",
       " 'street': True,\n",
       " '3': True,\n",
       " '7': True,\n",
       " '/': True,\n",
       " '10': True,\n",
       " 'blair': True,\n",
       " 'witch': True,\n",
       " '2': True,\n",
       " 'crow': True,\n",
       " '9': True,\n",
       " 'salvation': True,\n",
       " '4': True,\n",
       " 'stir': True,\n",
       " 'echoes': True,\n",
       " '8': True}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfeats['neg'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bag of words model을 이용한 feature 추출 두번째 방법**\n",
    "1. 각 문서를 먼저 word list와 label의 list로 변환\n",
    "2. feature 추출 대상이 되는 단어 집합 구성 - 이 때 전체 words들에 대한frequency data를 이용해 상위 n개의 단어만으로 feature 구성이 가능\n",
    "3. 각 문서에 대해 feature 추출 대상 단어들을 대상으로 해당 단어가 문서에 있는 지의 여부를 dictionary로 생성\n",
    "4. 생성된 feature와 label로 문서들에 대한 feature set을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "# category 별로 fileid를 추출하고, 해당 fileid에 대해 문서의 word tokenize된 결과를 가져와서 \n",
    "# documents 집합을 구성\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk FreqDist 함수를 이용하여 단어별로 빈도 수를 계산\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 77717\n",
      "the 76529\n",
      ". 65876\n",
      "a 38106\n",
      "and 35576\n",
      "of 34123\n",
      "to 31937\n",
      "' 30585\n",
      "is 25195\n",
      "in 21822\n"
     ]
    }
   ],
   "source": [
    "for word in list(all_words)[:10]:\n",
    "    print(word, all_words[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 77717\n",
      "the 76529\n",
      ". 65876\n",
      "a 38106\n",
      "and 35576\n",
      "of 34123\n",
      "to 31937\n",
      "' 30585\n",
      "is 25195\n",
      "in 21822\n",
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in']\n"
     ]
    }
   ],
   "source": [
    "sorted_features = sorted(all_words, key=all_words.get, reverse=True)\n",
    "for word in sorted_features[:10]:\n",
    "    print(word, all_words[word])\n",
    "print(sorted_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', '?', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', ':', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', ';', 'first', '--', 'after', 'see', '!', 'way', 'because', 'make', 'life']\n"
     ]
    }
   ],
   "source": [
    "word_features = sorted_features[:2000] #빈도가 높은 상위 2000개의 단어만 추출하여 features를 구성\n",
    "\n",
    "print(word_features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 document를 feature로 변환하는 함수, word_features를 사용\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words) #2000개의 단어에 대해 True, False로 입력\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에서 만든 documents 집합에 대해 feature set을 생성\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "pos\n",
      "{',': True, 'the': True, '.': True, 'a': True, 'and': True, 'of': True, 'to': True, \"'\": True, 'is': True, 'in': True, 's': True, '\"': True, 'it': True, 'that': True, '-': True, ')': True, '(': True, 'as': True, 'with': True, 'for': True, 'his': True, 'this': True, 'film': True, 'i': True, 'he': True, 'but': True, 'on': True, 'are': True, 't': True, 'by': True, 'be': True, 'one': True, 'movie': True, 'an': True, 'who': True, 'not': False, 'you': True, 'from': True, 'at': True, 'was': True, 'have': False, 'they': True, 'has': True, 'her': False, 'all': True, '?': True, 'there': False, 'like': True, 'so': True, 'out': True, 'about': True, 'up': True, 'more': True, 'what': True, 'when': False, 'which': True, 'or': True, 'she': True, 'their': True, ':': True, 'some': True, 'just': False, 'can': True, 'if': True, 'we': False, 'him': True, 'into': True, 'even': True, 'only': True, 'than': True, 'no': True, 'good': True, 'time': True, 'most': True, 'its': True, 'will': True, 'story': True, 'would': False, 'been': False, 'much': True, 'character': False, 'also': True, 'get': False, 'other': True, 'do': True, 'two': False, 'well': True, 'them': True, 'very': False, 'characters': True, ';': False, 'first': False, '--': False, 'after': False, 'see': False, '!': True, 'way': True, 'because': False, 'make': False, 'life': False, 'off': True, 'too': True, 'any': False, 'does': True, 'really': True, 'had': True, 'while': False, 'films': True, 'how': True, 'plot': True, 'little': True, 'where': True, 'people': False, 'over': False, 'could': False, 'then': True, 'me': True, 'scene': False, 'man': True, 'bad': True, 'my': True, 'never': True, 'being': True, 'best': False, 'these': False, 'don': False, 'new': True, 'doesn': True, 'scenes': False, 'many': False, 'director': True, 'such': False, 'know': True, 'were': True, 'movies': True, 'through': False, 'here': True, 'action': False, 'great': True, 're': False, 'another': True, 'love': False, 'go': False, 'made': False, 'us': False, 'big': False, 'end': True, 'something': False, 'back': False, '*': False, 'still': True, 'world': False, 'seems': False, 'work': False, 'those': False, 'makes': False, 'now': True, 'before': True, 'however': False, 'between': False, 'few': False, '/': True, 'down': True, 'every': True, 'though': False, 'better': True, 'real': False, 'audience': False, 'enough': False, 'seen': True, 'take': False, 'around': True, 'both': False, 'going': False, 'year': True, 'performance': False, 'why': False, 'should': False, 'role': False, 'isn': True, 'same': False, 'old': False, 'gets': True, 'your': True, 'may': False, 'things': False, 'think': True, 'years': False, 'last': False, 'comedy': False, 'funny': True, 'actually': True, 've': True, 'long': True, 'look': False, 'almost': False, 'own': False, 'thing': True, 'fact': False, 'nothing': False, 'say': False, 'right': True, 'john': True, 'although': False, 'played': False, 'find': False, 'script': False, 'come': False, 'ever': True, 'cast': False, 'since': True, 'did': True, 'star': False, 'plays': False, 'young': True, 'show': False, 'comes': False, 'm': True, 'part': False, 'original': False, 'actors': False, 'screen': False, 'without': False, 'again': True, 'acting': False, 'three': True, 'day': False, 'each': True, 'point': False, 'lot': False, 'least': False, 'takes': True, 'guy': True, 'quite': True, 'himself': False, 'away': True, 'during': True, 'family': False, 'effects': False, 'course': False, 'goes': False, 'minutes': False, 'interesting': False, 'might': False, 'far': False, 'high': False, 'rather': False, 'once': True, 'must': False, 'anything': True, 'place': False, 'set': False, 'yet': False, 'watch': False, 'd': True, 'making': False, 'our': False, 'wife': False, 'hard': False, 'always': True, 'fun': False, 'didn': True, 'll': True, 'seem': False, 'special': False, 'bit': True, 'times': False, 'trying': False, 'hollywood': False, 'instead': False, 'give': True, 'want': False, 'picture': False, 'kind': False, 'american': True, 'job': False, 'sense': False, 'woman': False, 'home': False, 'having': False, 'series': False, 'actor': False, 'probably': False, 'help': True, 'half': False, 'along': False, 'men': False, 'everything': True, 'pretty': True, 'becomes': False, 'sure': False, 'black': False, 'together': False, 'dialogue': False, 'money': False, 'become': False, 'gives': True, 'given': False, 'looking': False, 'whole': True, 'watching': False, 'father': False, '`': False, 'feel': False, 'everyone': True, 'music': False, 'wants': False, 'sex': False, 'less': False, 'done': False, 'horror': False, 'got': False, 'death': False, 'perhaps': False, 'city': False, 'next': True, 'especially': False, 'play': False, 'girl': True, 'mind': False, '10': True, 'moments': True, 'looks': True, 'completely': False, '2': False, 'reason': False, 'mother': False, 'whose': False, 'line': False, 'night': False, 'human': True, 'until': False, 'rest': False, 'performances': False, 'different': False, 'evil': False, 'small': True, 'james': False, 'simply': False, 'couple': False, 'put': True, 'let': True, 'anyone': False, 'ending': True, 'case': False, 'several': False, 'dead': False, 'michael': False, 'left': False, 'thought': True, 'school': False, 'shows': True, 'humor': False, 'true': False, 'lost': False, 'written': False, 'itself': False, 'friend': True, 'entire': False, 'getting': False, 'town': False, 'turns': False, 'soon': False, 'someone': True, 'second': False, 'main': True, 'stars': False, 'found': False, 'use': False, 'problem': False, 'friends': False, 'tv': False, 'top': False, 'name': False, 'begins': False, 'called': False, 'based': False, 'comic': False, 'david': False, 'head': False, 'else': True, 'idea': False, 'either': False, 'wrong': False, 'unfortunately': False, 'later': False, 'final': True, 'hand': False, 'alien': False, 'house': True, 'group': False, 'full': False, 'used': False, 'tries': False, 'often': False, 'against': False, 'war': False, 'sequence': False, 'keep': False, 'turn': True, 'playing': False, 'boy': True, 'behind': True, 'named': False, 'certainly': False, 'live': False, 'believe': False, 'under': False, 'works': False, 'relationship': False, 'face': True, 'hour': False, 'run': False, 'style': True, 'said': False, 'despite': False, 'person': False, 'finally': False, 'shot': False, 'book': False, 'doing': False, 'tell': False, 'maybe': True, 'nice': False, 'son': False, 'perfect': False, 'side': False, 'seeing': False, 'able': True, 'finds': False, 'children': False, 'days': False, 'past': False, 'summer': False, 'camera': False, 'won': False, 'including': False, 'mr': False, 'kids': False, 'lives': False, 'directed': False, 'moment': False, 'game': False, 'running': False, 'fight': False, 'supposed': False, 'video': False, 'car': True, 'matter': False, 'kevin': False, 'joe': False, 'lines': False, 'worth': False, '=': False, 'daughter': False, 'earth': False, 'starts': True, 'need': False, 'entertaining': False, 'white': False, 'start': False, 'writer': False, 'dark': False, 'short': False, 'self': False, 'worst': False, 'nearly': False, 'opening': False, 'try': False, 'upon': False, 'care': False, 'early': False, 'violence': False, 'throughout': False, 'team': False, 'production': False, 'example': False, 'beautiful': False, 'title': False, 'exactly': False, 'jack': False, 'review': False, 'major': False, 'drama': False, '&': False, 'problems': False, 'sequences': False, 'obvious': False, 'version': False, 'screenplay': False, 'known': False, 'killer': False, 'wasn': False, 'robert': False, 'disney': False, 'already': False, 'close': False, 'classic': False, 'others': True, 'hit': False, 'kill': False, 'deep': True, 'five': False, 'order': False, 'act': False, 'simple': False, 'fine': False, 'themselves': False, 'heart': False, 'roles': True, 'jackie': False, 'direction': False, 'eyes': False, 'four': False, 'question': False, 'sort': False, 'sometimes': False, 'knows': False, 'supporting': False, 'coming': True, 'voice': False, 'women': False, 'truly': False, 'save': False, 'jokes': False, 'computer': False, 'child': False, 'o': False, 'boring': False, 'tom': False, 'level': True, '1': False, 'body': False, 'guys': False, 'genre': False, 'brother': True, 'strong': False, 'stop': False, 'room': False, 'space': False, 'lee': False, 'ends': False, 'beginning': False, 'ship': False, 'york': False, 'attempt': False, 'thriller': False, 'scream': False, 'peter': False, 'aren': False, 'husband': False, 'fiction': False, 'happens': False, 'hero': False, 'novel': False, 'note': False, 'hope': False, 'king': False, 'yes': False, 'says': False, 'tells': False, 'quickly': False, 'romantic': False, 'dog': False, 'oscar': False, 'stupid': False, 'possible': False, 'saw': False, 'lead': True, 'career': False, 'murder': False, 'extremely': False, 'manages': True, 'god': False, 'mostly': False, 'wonder': True, 'particularly': False, 'future': False, 'fans': False, 'sound': False, 'worse': False, 'piece': True, 'involving': False, 'de': False, 'appears': False, 'planet': False, 'paul': True, 'involved': False, 'mean': False, 'none': False, 'taking': False, 'hours': False, 'laugh': False, 'police': False, 'sets': True, 'attention': False, 'co': False, 'hell': False, 'eventually': False, 'single': False, 'fall': False, 'falls': False, 'material': False, 'emotional': False, 'power': False, 'late': False, 'lack': False, 'dr': False, 'van': False, 'result': False, 'elements': False, 'meet': False, 'smith': False, 'science': False, 'experience': False, 'bring': True, 'wild': False, 'living': False, 'theater': False, 'interest': False, 'leads': False, 'word': False, 'feature': False, 'battle': False, 'girls': False, 'alone': False, 'obviously': False, 'george': False, 'within': False, 'usually': False, 'enjoy': False, 'guess': False, 'among': False, 'taken': False, 'feeling': False, 'laughs': False, 'aliens': False, 'talk': False, 'chance': False, 'talent': False, '3': False, 'middle': False, 'number': False, 'easy': False, 'across': False, 'needs': False, 'attempts': False, 'happen': False, 'television': False, 'chris': False, 'deal': False, 'poor': False, 'form': False, 'girlfriend': False, 'viewer': False, 'release': False, 'killed': False, 'forced': False, 'whether': False, 'wonderful': False, 'feels': False, 'oh': False, 'tale': False, 'serious': False, 'expect': False, 'except': False, 'light': False, 'success': False, 'features': False, 'premise': True, 'happy': False, 'words': False, 'leave': False, 'important': False, 'meets': False, 'history': False, 'giving': False, 'crew': False, 'type': True, 'call': False, 'turned': False, 'released': False, 'parents': False, 'art': False, 'impressive': False, 'mission': False, 'working': False, 'seemed': False, 'score': True, 'told': False, 'recent': False, 'robin': False, 'basically': False, 'entertainment': False, 'america': False, '$': False, 'surprise': False, 'apparently': False, 'easily': False, 'ryan': False, 'cool': False, 'stuff': False, 'cop': False, 'change': False, 'williams': False, 'crime': False, 'office': False, 'parts': False, 'somehow': False, 'sequel': False, 'william': False, 'cut': False, 'die': False, 'jones': False, 'credits': False, 'batman': False, 'suspense': True, 'brings': False, 'events': True, 'reality': False, 'whom': False, 'local': False, 'talking': False, 'difficult': False, 'using': False, 'went': False, 'writing': False, 'remember': False, 'near': False, 'straight': False, 'hilarious': False, 'ago': False, 'certain': False, 'ben': False, 'kid': False, 'wouldn': False, 'slow': False, 'blood': False, 'mystery': False, 'complete': False, 'red': True, 'popular': False, 'effective': False, 'am': False, 'fast': True, 'flick': True, 'due': False, 'runs': False, 'gone': False, 'return': False, 'presence': False, 'quality': False, 'dramatic': False, 'filmmakers': False, 'age': False, 'brothers': False, 'business': False, 'general': False, 'rock': True, 'sexual': False, 'present': False, 'surprisingly': False, 'anyway': False, 'uses': False, '4': True, 'personal': False, 'figure': False, 'smart': False, 'ways': False, 'decides': False, 'annoying': False, 'begin': False, 'couldn': True, 'somewhat': False, 'shots': False, 'rich': False, 'minute': True, 'law': False, 'previous': True, 'jim': False, 'successful': False, 'harry': False, 'water': False, 'similar': False, 'absolutely': False, 'motion': False, 'former': False, 'strange': False, 'came': False, 'follow': False, 'read': False, 'project': False, 'million': False, 'secret': False, 'starring': False, 'clear': False, 'familiar': False, 'romance': False, 'intelligent': False, 'third': False, 'excellent': True, 'amazing': True, 'party': False, 'budget': False, 'eye': True, 'actress': False, 'prison': False, 'latest': False, 'means': False, 'company': False, 'towards': False, 'predictable': False, 'powerful': False, 'nor': False, 'bob': False, 'beyond': False, 'visual': False, 'leaves': False, 'r': False, 'nature': False, 'following': True, 'villain': False, 'leaving': False, 'animated': False, 'low': False, 'myself': True, 'b': False, 'bill': False, 'sam': False, 'filled': False, 'wars': False, 'questions': False, 'cinema': False, 'message': False, 'box': False, 'moving': False, 'herself': False, 'country': True, 'usual': False, 'martin': False, 'definitely': False, 'add': False, 'large': False, 'clever': False, 'create': False, 'felt': False, 'stories': False, 'brilliant': False, 'ones': False, 'giant': False, 'situation': False, 'murphy': False, 'break': False, 'opens': False, 'scary': False, 'doubt': False, 'drug': False, 'bunch': False, 'thinking': False, 'solid': True, 'effect': False, 'learn': False, 'move': False, 'force': False, 'potential': False, 'seriously': False, 'follows': False, 'above': False, 'saying': False, 'huge': False, 'class': False, 'plan': False, 'agent': False, 'created': False, 'unlike': False, 'pay': False, 'non': False, 'married': False, 'mark': False, 'sweet': False, 'perfectly': False, 'ex': False, 'realize': False, 'audiences': False, 'took': True, 'decent': False, 'likely': True, 'dream': False, 'view': False, 'scott': False, 'subject': False, 'understand': False, 'happened': False, 'enjoyable': False, 'studio': False, 'immediately': False, 'open': False, 'e': False, 'points': False, 'heard': False, 'viewers': False, 'cameron': False, 'truman': False, 'bruce': False, 'frank': False, 'private': False, 'stay': False, 'fails': False, 'impossible': False, 'cold': False, 'richard': False, 'overall': False, 'merely': False, 'exciting': False, 'mess': False, 'chase': False, 'free': False, 'ten': False, 'neither': False, 'wanted': False, 'gun': False, 'appear': False, 'carter': False, 'escape': False, 'ultimately': False, '+': False, 'fan': False, 'inside': False, 'favorite': False, 'haven': True, 'modern': False, 'l': False, 'wedding': False, 'stone': False, 'trek': False, 'brought': True, 'trouble': True, 'otherwise': False, 'tim': False, '5': False, 'allen': False, 'bond': False, 'society': False, 'liked': False, 'dumb': False, 'musical': False, 'stand': False, 'political': False, 'various': False, 'talented': False, 'particular': False, 'west': True, 'state': False, 'keeps': False, 'english': False, 'silly': False, 'u': False, 'situations': True, 'park': False, 'teen': False, 'rating': False, 'slightly': False, 'steve': True, 'truth': False, 'air': False, 'element': False, 'joke': False, 'spend': False, 'key': False, 'biggest': False, 'members': False, 'effort': False, 'government': False, 'focus': False, 'eddie': False, 'soundtrack': False, 'hands': False, 'earlier': False, 'chan': False, 'purpose': False, 'today': False, 'showing': False, 'memorable': False, 'six': False, 'cannot': False, 'max': False, 'offers': False, 'rated': False, 'mars': False, 'heavy': False, 'totally': False, 'control': False, 'credit': False, 'fi': False, 'woody': False, 'ideas': False, 'sci': False, 'wait': False, 'sit': False, 'female': False, 'ask': False, 'waste': False, 'terrible': False, 'depth': True, 'simon': False, 'aspect': False, 'list': False, 'mary': False, 'sister': False, 'animation': False, 'entirely': False, 'fear': False, 'steven': False, 'moves': False, 'actual': False, 'army': False, 'british': False, 'constantly': False, 'fire': False, 'convincing': False, 'setting': False, 'gave': False, 'tension': True, 'street': False, '8': True, 'brief': False, 'ridiculous': False, 'cinematography': False, 'typical': False, 'nick': False, 'screenwriter': False, 'ability': False, 'spent': False, 'quick': False, 'violent': False, 'atmosphere': False, 'subtle': False, 'expected': False, 'fairly': False, 'seven': False, 'killing': False, 'tone': False, 'master': False, 'disaster': False, 'lots': False, 'thinks': False, 'song': False, 'cheap': False, 'suddenly': False, 'background': True, 'club': False, 'willis': False, 'whatever': False, 'highly': False, 'sees': False, 'complex': False, 'greatest': False, 'impact': False, 'beauty': False, 'front': False, 'humans': False, 'indeed': False, 'flat': False, 'grace': False, 'wrote': False, 'amusing': False, 'ii': False, 'mike': False, 'further': False, 'cute': False, 'dull': False, 'minor': False, 'recently': False, 'hate': False, 'outside': False, 'plenty': False, 'wish': False, 'godzilla': False, 'college': False, 'titanic': False, 'sounds': False, 'telling': False, 'sight': False, 'double': False, 'cinematic': False, 'queen': False, 'hold': False, 'meanwhile': False, 'awful': False, 'clearly': False, 'theme': False, 'hear': False, 'x': False, 'amount': False, 'baby': False, 'approach': False, 'dreams': False, 'shown': False, 'island': False, 'reasons': False, 'charm': False, 'miss': False, 'longer': False, 'common': False, 'sean': False, 'carry': False, 'believable': True, 'realistic': True, 'chemistry': False, 'possibly': False, 'casting': False, 'carrey': False, 'french': False, 'trailer': True, 'tough': False, 'produced': False, 'imagine': False, 'choice': False, 'ride': True, 'somewhere': False, 'hot': False, 'race': False, 'road': False, 'leader': False, 'thin': False, 'jerry': False, 'slowly': False, 'delivers': False, 'detective': False, 'brown': False, 'jackson': False, 'member': False, 'provide': False, 'president': False, 'puts': False, 'asks': False, 'critics': False, 'appearance': False, 'famous': False, 'okay': False, 'intelligence': False, 'energy': False, 'sent': False, 'spielberg': False, 'development': False, 'etc': False, 'language': False, 'blue': True, 'proves': False, 'vampire': False, 'seemingly': False, 'basic': False, 'caught': False, 'decide': True, 'opportunity': False, 'incredibly': False, 'images': False, 'band': False, 'j': False, 'writers': False, 'knew': False, 'interested': False, 'considering': False, 'boys': False, 'thanks': False, 'remains': False, 'climax': False, 'event': False, 'directing': False, 'conclusion': False, 'leading': False, 'ground': False, 'lies': False, 'forget': False, 'alive': False, 'tarzan': False, 'century': False, 'provides': False, 'trip': False, 'partner': False, 'central': False, 'tarantino': False, 'period': False, 'pace': True, 'yourself': True, 'worked': False, 'ready': False, 'date': False, 'thus': False, '1998': False, 'terrific': False, 'write': False, 'average': False, 'onto': False, 'songs': False, 'occasionally': False, 'doctor': False, 'stands': False, 'hardly': False, 'monster': False, 'led': False, 'mysterious': False, 'details': False, 'wasted': False, 'apart': False, 'aside': False, 'store': False, 'billy': False, 'boss': False, 'travolta': False, 'producer': False, 'pull': False, 'consider': False, 'pictures': False, 'becoming': False, 'cage': False, 'loud': False, 'looked': False, 'officer': False, 'twenty': False, 'system': False, 'contains': False, 'julia': False, 'subplot': False, 'missing': False, 'personality': False, 'building': False, 'learns': False, 'hong': False, 'la': False, 'apartment': False, '7': True, 'bizarre': False, 'powers': False, 'flaws': False, 'catch': True, 'lawyer': False, 'shoot': False, 'student': False, 'unique': False, '000': False, 'admit': False, 'concept': False, 'needed': False, 'thrown': False, 'christopher': False, 'laughing': False, 'green': False, 'twists': False, 'matthew': False, 'touch': False, 'waiting': False, 'victim': False, 'cover': False, 'machine': False, 'danny': False, 'mention': False, 'search': False, '1997': False, 'win': False, 'door': False, 'manner': False, 'train': False, 'saving': False, 'share': False, 'image': False, 'discovers': False, 'normal': False, 'cross': True, 'fox': False, 'returns': False, 'adult': False, 'adds': False, 'answer': False, 'adventure': False, 'lame': False, 'male': False, 'odd': False, 'singer': False, 'deserves': False, 'gore': False, 'states': False, 'include': False, 'equally': False, 'months': False, 'barely': False, 'directors': False, 'introduced': False, 'fashion': False, 'social': False, '1999': False, 'news': False, 'hair': False, 'dance': False, 'innocent': False, 'camp': False, 'teacher': False, 'became': False, 'sad': False, 'witch': False, 'includes': False, 'nights': False, 'jason': False, 'julie': False, 'latter': False, 'food': False, 'jennifer': False, 'land': False, 'menace': False, 'rate': False, 'storyline': False, 'contact': False, 'jean': False, 'elizabeth': False, 'fellow': False, 'changes': False, 'henry': False, 'hill': False, 'pulp': False, 'gay': False, 'tried': False, 'surprised': True, 'literally': False, 'walk': False, 'standard': False, '90': False, 'forward': False, 'wise': True, 'enjoyed': False, 'discover': False, 'pop': False, 'anderson': False, 'offer': False, 'recommend': False, 'public': False, 'drive': False, 'c': False, 'toy': False, 'charming': False, 'fair': False, 'chinese': False, 'rescue': False, 'terms': False, 'mouth': False, 'lucas': False, 'accident': False, 'dies': False, 'decided': False, 'edge': False, 'footage': False, 'culture': False, 'weak': False, 'presented': False, 'blade': False, 'younger': False, 'douglas': False, 'natural': False, 'born': False, 'generally': False, 'teenage': False, 'older': False, 'horrible': False, 'addition': False, 'sadly': False, 'creates': False, 'disturbing': False, 'roger': False, 'detail': False, 'devil': False, 'debut': False, 'track': True, 'developed': False, 'week': False, 'russell': False, 'attack': False, 'explain': False, 'rarely': False, 'fully': False, 'prove': False, 'exception': False, 'jeff': False, 'twist': False, 'gang': False, 'winning': False, 'jr': False, 'species': False, 'issues': False, 'fresh': False, 'rules': False, 'meaning': False, 'inspired': False, 'heroes': False, 'desperate': True, 'fighting': False, 'filmed': False, 'faces': False, 'alan': False, 'bright': False, 'ass': False, 'flying': False, 'kong': False, 'rush': False, 'forces': False, 'charles': False, 'numerous': False, 'emotions': False, 'involves': False, 'patrick': False, 'weird': False, 'apparent': False, 'information': False, 'revenge': False, 'jay': False, 'toward': False, 'surprising': False, 'twice': False, 'editing': True, 'calls': False, 'lose': False, 'vegas': False, 'stage': False, 'intended': False, 'gags': False, 'opinion': False, 'likes': False, 'crazy': False, 'owner': False, 'places': False, 'pair': False, 'genuine': False, 'epic': False, 'speak': False, 'throw': False, 'appeal': False, 'gibson': False, 'captain': False, 'military': False, '20': False, 'blair': False, 'nowhere': False, 'length': False, 'nicely': False, 'cause': False, 'pass': False, 'episode': False, 'kiss': False, 'arnold': False, 'please': False, 'hasn': False, 'phone': False, 'filmmaking': False, 'formula': False, 'boyfriend': False, 'talents': False, 'creating': False, 'kelly': False, 'buy': False, 'wide': False, 'fantasy': False, 'mood': False, 'heads': False, 'pathetic': False, 'lacks': False, 'loved': True, 'asked': False, 'mrs': False, 'witty': False, 'shakespeare': False, 'mulan': False, 'generation': False, 'affair': False, 'pieces': False, 'task': False, 'rare': False, 'kept': False, 'cameo': False, 'fascinating': False, 'ed': False, 'fbi': False, 'burton': False, 'incredible': False, 'accent': False, 'artist': False, 'superior': False, 'academy': False, 'thomas': False, 'spirit': False, 'technical': False, 'confusing': False, 'poorly': False, 'target': False, 'lover': False, 'woo': False, 'mentioned': False, 'theaters': False, 'plane': False, 'confused': False, 'dennis': False, 'rob': False, 'appropriate': False, 'christmas': False, 'considered': False, 'legend': False, 'shame': False, 'soul': False, 'matt': False, 'campbell': False, 'process': False, 'bottom': False, 'sitting': False, 'brain': False, 'creepy': False, '13': False, 'forever': False, 'dude': True, 'crap': False, 'superb': False, 'speech': False, 'ice': False, 'journey': False, 'masterpiece': False, 'intriguing': False, 'names': False, 'pick': True, 'speaking': True, 'virtually': False, 'award': False, 'worthy': False, 'marriage': False, 'deliver': True, 'cash': False, 'magic': False, 'respect': False, 'product': False, 'necessary': False, 'suppose': False, 'silent': False, 'pointless': False, 'station': False, 'affleck': False, 'dimensional': False, 'charlie': False, 'allows': False, 'avoid': False, 'meant': False, 'cops': False, 'attitude': False, 'relationships': False, 'hits': False, 'stephen': False, 'spends': False, 'relief': False, 'physical': False, 'count': False, 'reviews': False, 'appreciate': True, 'cliches': False, 'holds': False, 'pure': False, 'plans': False, 'limited': False, 'failed': False, 'pain': False, 'impression': False, 'unless': False, 'sub': False, '[': False, 'total': False, 'creature': False, 'viewing': False, 'loves': False, 'princess': False, 'kate': False, 'rising': False, 'woods': False, 'baldwin': False, 'angry': False, 'drawn': False, 'step': False, 'matrix': False, 'themes': False, 'satire': False, 'arts': False, ']': False, 'remake': False, 'wall': False, 'moral': False, 'color': False, 'ray': False, 'stuck': True, 'touching': False, 'wit': False, 'tony': False, 'hanks': False, 'continues': True, 'damn': True, 'nobody': False, 'cartoon': False, 'keeping': False, 'realized': False, 'criminal': False, 'unfunny': False, 'comedic': False, 'martial': False, 'disappointing': False, 'anti': False, 'graphic': False, 'stunning': False, 'actions': False, 'floor': False, 'emotion': False, 'soldiers': False, 'edward': False, 'comedies': False, 'driver': False, 'expectations': False, 'added': False, 'mad': False, 'angels': False, 'shallow': False, 'suspect': False, 'humorous': False, 'phantom': False, 'appealing': False, 'device': False, 'design': False, 'industry': False, 'reach': False, 'fat': False, 'blame': False, 'united': False, 'sign': False, 'portrayal': False, 'rocky': False, 'finale': False, 'grand': False, 'opposite': False, 'hotel': False, 'match': False, 'damme': False, 'speed': False, 'ok': False, 'loving': False, 'field': False, 'larry': False, 'urban': False, 'troopers': False, 'compared': False, 'apes': False, 'rose': False, 'falling': False, 'era': False, 'loses': False, 'adults': False, 'managed': False, 'dad': False, 'therefore': False, 'pg': False, 'results': False, 'guns': False, 'radio': True, 'lady': False, 'manage': False, 'spice': False, 'naked': False, 'started': False, 'intense': False, 'humanity': False, 'wonderfully': False, 'slasher': False, 'bland': False, 'imagination': False, 'walking': False, 'willing': False, 'horse': False, 'rent': True, 'mix': False, 'generated': False, 'g': False, 'utterly': False, 'scientist': False, 'washington': False, 'notice': False, 'players': False, 'teenagers': False, 'moore': False, 'board': False, 'price': False, 'frightening': False, 'tommy': False, 'spectacular': False, 'bored': False, 'jane': False, 'join': False, 'producers': False, 'johnny': False, 'zero': False, 'vampires': False, 'adaptation': False, 'dollars': False, 'parody': False, 'documentary': False, 'dvd': False, 'wayne': False, 'post': False, 'exist': False, 'matters': False, 'chosen': False, 'mel': False, 'attractive': False, 'plain': False, 'trust': False, 'safe': False, 'reading': False, 'hoping': False, 'protagonist': False, 'feelings': False, 'fate': False, 'finding': False, 'feet': False, 'visuals': False, 'spawn': False, 'compelling': False, 'hall': False, 'sympathetic': False, 'featuring': False, 'difference': False, 'professional': False, 'drugs': False, 'ford': False, 'shooting': False, 'gold': False, 'patch': False, 'build': False, 'boat': False, 'cruise': False, 'honest': False, 'media': False, 'flicks': True, 'bug': False, 'bringing': False, 'dangerous': False, 'watched': False, 'grant': False, 'smile': False, 'plus': False, 'shouldn': False, 'decision': False, 'visually': False, 'allow': False, 'starship': False, 'roberts': False, 'dying': False, 'portrayed': False, 'turning': False, 'believes': False, 'changed': False, 'shock': False, 'destroy': False, '30': False, 'crowd': False, 'broken': False, 'tired': False, 'fail': False, 'south': False, 'died': False, 'cult': False, 'fake': False, 'vincent': False, 'identity': False, 'sexy': False, 'hunt': False, 'jedi': False, 'flynt': False, 'alex': False, 'engaging': False, 'serve': False, 'snake': False, 'yeah': False, 'expecting': False, '100': False, 'decade': False, 'ups': False, 'constant': False, 'current': False, 'survive': False, 'jimmy': False, 'buddy': False, 'send': False, 'brooks': False, 'goofy': False, 'likable': False, 'humour': False, 'technology': False, 'files': False, 'babe': False, 'aspects': False, 'presents': False, 'kills': False, 'supposedly': False, 'eight': False, 'sandler': False, 'hospital': False, 'test': False, 'hidden': False, 'brian': False, 'books': False, 'promise': False, 'determined': False, 'professor': False, 'welcome': False, 'pleasure': False, 'succeeds': False, 'individual': False, 'annie': False, 'mob': False, 'ted': False, 'virus': False, 'content': False, 'gary': False, 'direct': False, 'contrived': False, 'carpenter': False, 'scale': False, 'sick': False, 'nasty': False, 'conflict': False, 'haunting': False, 'ghost': False, 'filmmaker': False, 'japanese': False, 'helps': False, 'fare': False, 'lucky': False, 'ultimate': False, 'window': False, 'support': False, 'goal': False, 'provided': False, 'genius': False, 'winner': False, 'taylor': False, 'fantastic': False, 'faith': False, 'lynch': False, 'fit': False, 'catherine': False, 'ms': False, 'paced': False, 'breaks': False, 'al': False, 'frame': False, 'travel': False, 'badly': False, 'available': False, 'cares': False, 'reeves': False, 'crash': False, 'driving': True, 'press': False, 'seagal': False, 'amy': False, '9': False, 'headed': False, 'instance': False, 'excuse': False, 'offensive': False, 'narrative': False, 'fault': False, 'bus': False, 'f': False, 'extreme': False, 'miller': False, 'guilty': False, 'grows': False, 'overly': False, 'liners': False, 'forgotten': False, 'ahead': False, 'accept': False, 'porn': False, 'directly': False, 'helen': False, 'began': False, 'lord': False, 'folks': False, 'mediocre': False, 'bar': False, 'surface': False, 'super': False, 'failure': False, '6': True, 'acted': False, 'quiet': False, 'laughable': False, 'sheer': False, 'security': False, 'emotionally': False, 'season': False, 'stuart': False, 'jail': False, 'deals': False, 'cheesy': False, 'court': False, 'beach': False, 'austin': False, 'model': False, 'outstanding': False, 'substance': False, 'nudity': False, 'slapstick': False, 'joan': False, 'reveal': False, 'placed': False, 'check': False, 'beast': False, 'hurt': False, 'bloody': False, 'acts': False, 'fame': False, 'meeting': False, 'nuclear': False, '1996': False, 'strength': False, 'center': False, 'funniest': False, 'standing': False, 'damon': False, 'clich': False, 'position': False, 'desire': False, 'driven': False, 'seat': False, 'stock': False, 'wondering': False, 'realizes': False, 'dealing': False, 'taste': False, 'routine': False, 'comparison': False, 'cinematographer': False, 'seconds': False, 'singing': False, 'gangster': False, 'responsible': False, 'football': False, 'remarkable': False, 'hunting': False, 'adams': False, 'fly': False, 'suspects': False, 'treat': False, 'hopes': False, 'heaven': False, 'myers': False, 'struggle': False, 'costumes': False, 'beat': False, 'happening': False, 'skills': False, 'ugly': False, 'figures': False, 'thoroughly': True, 'ill': False, 'surprises': False, 'player': False, 'rival': False, 'guard': False, 'anthony': False, 'strike': False, 'community': False, 'streets': False, 'hopkins': False, 'ended': False, 'originally': False, 'sarah': False, 'creative': False, 'characterization': False, 'thankfully': False, 'growing': False, 'sharp': False, 'williamson': False, 'eccentric': False, 'explained': False, 'hey': False, 'claire': False, 'steal': False, 'inevitable': False, 'joel': False, 'core': False, 'weren': False, 'sorry': False, 'built': False, 'anne': False, 'breaking': False, 'villains': False, 'critic': False, 'lets': False, 'visit': False, 'followed': False}\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets[0][0])) #첫째 feature set의 첫째 element 즉 bag_of_words feature의 수 - 상위 2000개 단어\n",
    "print(featuresets[0][1]) #첫째 feature set의 둘째 element 즉 label\n",
    "print(featuresets[0][0]) # 첫째 feature set의 내용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW feature set을 이용한 분류 맛보기\n",
    "https://www.nltk.org/book/ch06.html Section 1.3\n",
    "\n",
    "![supervised-classification](supervised-classification.png)\n",
    "\n",
    "NLTK Naive Bayes 분류기를 사용 http://www.nltk.org/howto/classify.html\n",
    "주어진 문서의 label이 'neg'인지, 'pos'인지를 판단하는 분류기를 학습\n",
    "상세한 내용은 Text Classification에서!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.3 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.4 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.7 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.4 : 1.0\n",
      "                   damon = True              pos : neg    =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100] #train set과 test set으로 분리\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # train set으로 학습\n",
    "print(nltk.classify.accuracy(classifier, test_set)) # test set으로 분류기 성능을 평가\n",
    "classifier.show_most_informative_features(5) #분류기에서 가장 중요한 영향을 미치는 단어 상위 5개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "pos\n",
      "neg\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "testfeat = bag_of_words(['the', 'actor', 'was', 'seagal']) \n",
    "print(classifier.classify(testfeat))\n",
    "testfeat = bag_of_words(['the', 'story', 'was', 'outstanding'])\n",
    "print(classifier.classify(testfeat))\n",
    "negfeat = bag_of_words(['the', 'plot', 'was', 'ludicrous']) # 터무니없는\n",
    "print(classifier.classify(negfeat))\n",
    "posfeat = bag_of_words(['kate', 'winslet', 'is', 'accessible']) # 이해가 돼!\n",
    "print(classifier.classify(posfeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vector with Scikit\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 준비, 입력이 raw text임\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(vocabulary=[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is',\n",
      "                            'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as',\n",
      "                            'with', 'for', 'his', 'this', 'film', 'i', 'he',\n",
      "                            'but', 'on', 'are', 't', 'by', ...])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "cv = CountVectorizer(vocabulary=word_features) #빈도수 상위 2,000개의 단어만 사용하여 count vector 객체를 생성\n",
    "print(cv) #객체 parameter들을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews count: 2000\n"
     ]
    }
   ],
   "source": [
    "print('reviews count:', len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'a', 'and', 'of', 'to', \"'\", 'is', 'in', 's', '\"', 'it', 'that', '-', ')', '(', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', '?', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', ':', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', ';', 'first', '--', 'after', 'see', '!', 'way', 'because', 'make', 'life']\n"
     ]
    }
   ],
   "source": [
    "X = cv.fit_transform(reviews) #review를 이용하여 count vector를 학습하고, 변환\n",
    "print(cv.get_feature_names()[:100]) # count vector에 사용된 feature 이름을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 38  0  0 20 16 16  0 12  8  0  0 25 13  0  0  0  1  5  4  1 10  6  0\n",
      "  1 10  4 13  0  2  1  3  6  3  3  3  3  4  0  0  2  5  3  4  6  0 10  3\n",
      "  3  3  2  2  2  4  1  4  2  0  0  0  0  4  0  0  4  1  5  3  1  0  1  2\n",
      "  0  4  4  0  0  1  2  0  2  1  3  0  2  2  1  0  2  1  0  0  0  2  2  0\n",
      "  3  2  5  1]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(X[0].toarray()[0, :100]) #변환된 결과의 첫째 feature set 중에서 앞 100개를 출력\n",
    "print(max(X[0].toarray()[0])) #변환된 결과의 첫째 feature set 중에서 max 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", : 0\n",
      "the : 38\n",
      ". : 0\n",
      "a : 0\n",
      "and : 20\n",
      "of : 16\n",
      "to : 16\n",
      "' : 0\n",
      "is : 12\n",
      "in : 8\n",
      "s : 0\n",
      "\" : 0\n",
      "it : 25\n",
      "that : 13\n",
      "- : 0\n",
      ") : 0\n",
      "( : 0\n",
      "as : 1\n",
      "with : 5\n",
      "for : 4\n"
     ]
    }
   ],
   "source": [
    "for word, count in zip(cv.get_feature_names()[:20], X[0].toarray()[0, :20]):\n",
    "    print(word, ':', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 간 유사도 계산 및 가장 유사한 문서 검색\n",
    "\n",
    "### **Cosine similarity**\n",
    "\n",
    "(Wikipedia) a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.\n",
    "\n",
    "![cosine](cosine.png)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(0,2*np.pi,0.1)   # start,stop,step\n",
    "y = np.cos(x)\n",
    "#print(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "new_review = \"first of all it's a plot heavy mess that has bad voice talents , badly written script and fantastic animation. they are small pokemon with a powerful punch and have great psychic abilities\"\n",
    "new_vec = cv.transform([new_review]) #문서를 count vector로 변환\n",
    "\n",
    "sim_result = cosine_similarity(new_vec, X) #변환된 count vector와 기존 값들과의 similarity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47404152357849716, 0.4354705693078365, 0.4179484687443539, 0.40669013197195214, 0.4042441311205991, 0.4031257527594671, 0.39316730570210306, 0.38744458895341755, 0.3852303002224743, 0.384468719848977]\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/howto/sorting.html\n",
    "print(sorted(sim_result[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(sim_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" pokemon 3 : the movie \" has a lot of bad things in it . \n",
      "first of all it's a plot heavy mess that has bad voice talents , badly written script and fantastic animation . \n",
      "the first film came out the end of 1999 and was a huge hit grossing almost $90 million domestically . \n",
      "a sequel soon followed and even made $45 million . \n",
      "warner has released their third movie based on the immensely popular video game and tv series and its a waste of time and celluloid . \n",
      "this time ash ketchum and his friends are on their way to the johto battles ( which my little brother told me the new spinoff is \" pokemon : the johto journeys \" so go figure ) anyway he comes in contact with a young girl who's father has disappeared after trying to discover the unown . \n",
      "they are small pokemon with a powerful punch and have great psychic abilities . \n",
      "the unown bring together their psychic abilities and create entei a powerful legendary pokemon who barriers young molly's house and creates every wish she wants . \n",
      "now it's up to ash and his friends to stop this pokemon entei and show him to be a good pokemon rather than a bad one . \n",
      "too bad really that this is a bad movie , surprisingly the first movie was entertaining and somewhat absorbing , the second was a piece of trash and this one is almost in between . \n",
      "it has some good qualities ( animation , message in the end ) but the flaws seem to overpower the goods . \n",
      "i'm still not sure what the big thing is about pokemon , they are ugly little animals who speak their own name for their language ( besides meowth , my personal favorite ) and you don't understand what they are saying . \n",
      "my little brother just thought the movie was amazing , and i kept leaning over and asking him happened , or what pokemon that was . \n",
      "his response was a big lecture of how this is that , and that is this . . . he \n",
      "sure did put me in my place . \n",
      "with the second and third movie being bad , i have a feeling pokemon 4 : the movie might be a total bust as well . \n",
      " \" pokemon 3 : the movie \" has some redeeming qualities for the kids , and the pokemon fans will dig every minute of this film . \n",
      "for those parents and/or brothers and sisters who have to sit through this . . . bring a pillow . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reviews[679])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 679,  176, 1152, 1575,  952,  470,  688, 1144,  103,  159],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-sim_result[0]).argsort()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF with Scikit\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extractionalt \n",
    "\n",
    "![alt text](tfidf.png)\n",
    "![alt text](idf.png)\n",
    "\n",
    "기존에 만든 count vector로부터 TFIDF vector로 변환\n",
    "Count vector를 거치지 않고 처음부터 TFIDF vector를 생성하는 것도 가능함 -> 나중에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n",
      "max count score of the first vector: 38\n",
      "max tfidf score of the first vector: 0.3958279594831942\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = transformer.fit_transform(X)\n",
    "print(X_tfidf.shape)\n",
    "print('max count score of the first vector:', max(X[0].toarray()[0]))\n",
    "print('max tfidf score of the first vector:', max(X_tfidf[0].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tfidf = transformer.transform(new_vec)\n",
    "sim_result_tf = cosine_similarity(new_tfidf, X_tfidf)\n",
    "np.argmax(sim_result_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39386919931415737, 0.2158533727955655, 0.2055888622850777, 0.18940610744482467, 0.18862790397285573, 0.18841943811025338, 0.18329868368380425, 0.1810784645715442, 0.17853711709135542, 0.17620991687411133]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(sim_result_tf[0], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vector: [ 679  176 1152 1575  952  470  688 1144  103  159]\n",
      "TFIDF vector: [ 679  577 1209 1933  672 1280    3  913 1596 1163]\n"
     ]
    }
   ],
   "source": [
    "#count vector에 대한 유사도 상위 문서와 tfidf에 대한 유사도 상위 문서를 비교\n",
    "print('Count vector:', (-sim_result[0]).argsort()[:10])\n",
    "print('TFIDF vector:', (-sim_result_tf[0]).argsort()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Jaccard Similarity**\n",
    "https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied: sklearn in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\soo hyun\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "tfidf_list = [679, 1280,  577, 1933, 1209, 1115, 1274, 913, 1456, 1796]\n",
    "for i in tfidf_list:\n",
    "    print(jaccard_similarity_score(new_vec.toarray()[0], X[i].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension을 줄이는 방법: SVD\n",
    "lasso가 feature selection이라면, SVD는 feature extraction\n",
    "\n",
    "**Feature selection** (https://en.wikipedia.org/wiki/Feature_selection) \n",
    "\n",
    "In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "* simplification of models to make them easier to interpret by researchers/users,\n",
    "* shorter training times,\n",
    "* to avoid the curse of dimensionality,\n",
    "* enhanced generalization by reducing overfitting (formally, reduction of variance)\n",
    "\n",
    "Text mining에서는 단어의 수를 다양한 방법으로 줄일 수 있음\n",
    "\n",
    "* 전처리 과정에서 의미 없는 단어 삭제\n",
    "* stopwords 삭제\n",
    "* 빈도가 낮은 단어 삭제\n",
    "* 빈도가 높은 단어 삭제\n",
    "* 결과에 영향을 미치지 않는 단어 삭제\n",
    "\n",
    "**Feature extraction**: (https://en.wikipedia.org/wiki/Feature_extraction)\n",
    "\n",
    "In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.\n",
    "\n",
    "* PCA(Principal component analysis)\n",
    "* LSA(Latent semantic analysis)\n",
    "* SVD(Singular-value decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01514489 0.0166241  0.01317354 0.01030468 0.00756888 0.00725983\n",
      " 0.00659312 0.00580567 0.00550706 0.00504246 0.00483872 0.00470389\n",
      " 0.00453713 0.0043536  0.00431135 0.00415999 0.004104   0.00402205\n",
      " 0.00382987 0.00380754 0.0036883  0.00367408 0.00363124 0.00352173\n",
      " 0.00347459 0.00339472 0.00335051 0.00325916 0.00323815 0.00315384\n",
      " 0.00314674 0.00304913 0.00300347 0.00299865 0.00295145 0.00292693\n",
      " 0.00289485 0.00287923 0.00282802 0.00280223 0.00277773 0.00274264\n",
      " 0.00271311 0.00268979 0.00265569 0.00264501 0.00259901 0.00257714\n",
      " 0.0025622  0.00254445 0.00249797 0.00245484 0.00243712 0.00241656\n",
      " 0.00238654 0.00235366 0.00234395 0.00233282 0.00229758 0.002295\n",
      " 0.00227465 0.00226673 0.00224546 0.00223066 0.00221365 0.00220156\n",
      " 0.00219036 0.00218628 0.00216656 0.00216035 0.00215562 0.00211927\n",
      " 0.00211044 0.00207907 0.00207117 0.00205964 0.00205848 0.00203116\n",
      " 0.00202441 0.00199946 0.00199753 0.00199207 0.00197294 0.0019617\n",
      " 0.0019585  0.00193714 0.00191767 0.00191458 0.00189811 0.00186771\n",
      " 0.00185353 0.00184663 0.00183936 0.00182079 0.00180437 0.00178908\n",
      " 0.00177404 0.00176876 0.00174958 0.00173811]\n",
      "0.3301294081280183\n",
      "[29.54936635  4.36420045  3.88223926  3.4336061   2.9432501   2.88264756\n",
      "  2.74664301  2.57756775  2.51054801  2.40189889  2.35317037  2.3198506\n",
      "  2.27889108  2.23179972  2.22167603  2.1816127   2.16713559  2.14527806\n",
      "  2.0932693   2.0872057   2.05431727  2.05033882  2.03837721  2.0072858\n",
      "  1.99390129  1.97095962  1.95792141  1.93100751  1.92478453  1.89965894\n",
      "  1.89741067  1.86775163  1.85373119  1.85222777  1.83759385  1.82997386\n",
      "  1.81990837  1.81497947  1.79875771  1.79060839  1.78279738  1.77145545\n",
      "  1.76188644  1.75434521  1.74313166  1.73962107  1.72447955  1.71713989\n",
      "  1.71222068  1.70619451  1.69053883  1.67588603  1.66985071  1.66281531\n",
      "  1.65241505  1.641018    1.6376141   1.633811    1.62131504  1.62043129\n",
      "  1.61322709  1.61039551  1.60282223  1.59752576  1.59172378  1.58707378\n",
      "  1.58303285  1.58159752  1.57460225  1.57221768  1.57049958  1.55739933\n",
      "  1.55388822  1.54230728  1.5394555   1.53506889  1.5346513   1.52441458\n",
      "  1.52188451  1.51253005  1.51175704  1.50967713  1.50242339  1.49812574\n",
      "  1.49690923  1.48872122  1.48122565  1.48002615  1.47364228  1.46183106\n",
      "  1.45625308  1.45355162  1.45067424  1.44333001  1.43679407  1.43069132\n",
      "  1.42471488  1.42255847  1.41482775  1.41022141]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_tfidf)  \n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "print(svd.singular_values_)  \n",
    "newX = svd.transform(X_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2000)\n",
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(svd.components_.shape)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        ,  0.63007194,  0.        , ...,  0.00265981,\n",
       "        0.00257053,  0.00246303])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 영문문서 분류\n",
    "## Text Classification\n",
    "텍스트 문서의 다양한 분류방법에 대해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비 (복습)\n",
    "이전에 사용한 영화리뷰 데이터를 이용해서 주어진 리뷰 내용에 대해 positive와 negative를 분류하는 분류기를 학습하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews count: 2000\n",
      "Length of the first review: 4043\n",
      "Labels: {'pos', 'neg'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] \n",
    "#file id를 이용해 label로 사용할 category 즉 positive와 negative 정보를 순서대로 가져옴\n",
    "\n",
    "print('Reviews count:', len(reviews))\n",
    "print('Length of the first review:', len(reviews[0]))\n",
    "print('Labels:', set(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set과 test set의 분리\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "적절한 비율로 train set과 test set을 분리하여 저장<br>\n",
    "train set은 학습에 사용되고, test set은 검증에 사용<br>\n",
    "default로 shuffle을 함: train set과 test set이 고르게 분포되도록 하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=10)\n",
    "# sklearn의 train_test_split 함수는 먼저 data set을 shuffle하고 주어진 비율에 따라 train set과 test set을 나눠 줌\n",
    "# 위에서는 reviews를 X_train과 X_test로 8:2의 비율로 나누고, categories를 y_train과 y_test로 나눔\n",
    "# 이 때 X와 y의 순서는 동일하게 유지해서 각 입력값과 label이 정확하게 match되도록 함\n",
    "# random_state는 shuffle에서의 seed 값으로, 지정한 경우 항상 동일한 결과로 shuffle이 됨\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF 변환\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    \n",
    "CountVectorizer로 Count Vector를 생성하고 TFIDF로 변환하는 대신, text로부터 직접 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn에서 제공하는 TfidfVectorizer를 이용\n",
    "tfidf = TfidfVectorizer().fit(X_train) # X_train을 이용하여 vectorizer를 학습\n",
    "tfidf #vectorize에서 사용한 매개변수 값들을 확인 -> 현재는 모두 default 값을 사용, 향후 tokenizer, max_features 등을 지정할 수 있음\n",
    "# 상세한 매개변수 내용은 위 링크를 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 36310)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) #학습된 vectorizer를 이용하여 train set을 변환\n",
    "X_train_tfidf.shape # 1600 (review 수) x 36310 (전체 corpus에서 사용된 단어의 수) 크기로 vector set이 생성됨\n",
    "# matrix 안의 값은 해당 tfidf score임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=2000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000).fit(X_train) #사용된 단어의 수가 너무 많은 경우, max_feature를 제한하여 학습이 가능\n",
    "tfidf #max_factures 값이 사용된 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_tfidf.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayse Classifier (Scikit)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1600, 2000)\n",
      "Test set dimension: (400, 2000)\n"
     ]
    }
   ],
   "source": [
    "#나이브 베이즈는 word count를 사용하므로 tfdif가 아닌 count vectorizer를 사용하여 학습 및 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "NB_clf = MultinomialNB() # 분류기 선언\n",
    "\n",
    "NB_clf.fit(X_train_cv, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "#NB_clf.fit(X_train_tfidf, y_train) #tfidf 값을 사용할 수도 있으나, NB 이론에 맞지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.864\n",
      "Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test))) #test set에 대한 예측정확도를 확인\n",
    "#실제로 필요한 것은 test set에 대한 예측정확도이나, 과적합 (overfitting)의 문제가 있는지를 보기 위해 train set에 대한 예측정확도를 같이 확인\n",
    "#print('Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "#print('Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg' 'pos']\n",
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "#여러 문장에 대해 count vectorier로 변환 후 학습된 분류기로 결과를 예측\n",
    "print(NB_clf.predict(cv.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible'])))\n",
    "\n",
    "#위 첫째 문장에서 story를 actor로 바꿔서 예측\n",
    "print(NB_clf.predict(cv.transform(['the actor was unimaginative'])))\n",
    "#동일한 형용사라도 대상에 따라 결과가 바뀔 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Scikit)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "예측하고자 하는 값 혹은 label이 연속적인 값이 아니고 분류(class)일 때 사용하는 regression 방법<br>\n",
    "분류는 binary인 경우와 multi-class인 경우가 있음<br>\n",
    "지금은 positive와 negative 두 class 중에서 선택하므로 binary classification 문제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 1.000\n",
      "Test set score: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soo hyun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf_cv = LogisticRegression() #분류기 선언\n",
    "LR_clf_cv.fit(X_train_cv, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf_cv.score(X_train_cv, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf_cv.score(X_test_cv, y_test))) # test data에 대한 예측정확도\n",
    "# count vector를 이용한 regression 결과가 tfidf보다 더 좋게 나옴\n",
    "# 보통은 tfidf가 더 좋은 결과를 보이는데, 이와 같이 상황에 따라 다른 결과가 나오기도 함\n",
    "# 지금은 train data의 수가 1,600개인데 비해, 추정해야 하는 parameter의 수가 2,000개로 sample 수가 학습에 부족한 상황, \n",
    "# 따라서 예상 못한 다양한 결과가 나올 수 있음\n",
    "# 좀더 data가 많은 상황에서의 test가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.917\n",
      "Test set score: 0.820\n"
     ]
    }
   ],
   "source": [
    "#tfidf vector를 이용해서 분류기 학습\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도\n",
    "# NB에 비해 더 좋은 결과가 나오는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'neg', 'pos'], dtype='<U3')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB 분류기에서 사용했던 예제로 결과 확인, 실제로 결과가 더 나아졌음을 확인할 수 있음\n",
    "LR_clf.predict(tfidf.transform(['the story was unimaginative', 'the plot was ludicrous', 'kate winslet is accessible']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "https://brilliant.org/wiki/ridge-regression/\n",
    "Tikhonov Regularization, colloquially known as **ridge regression**, is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution. This type of problem is very common in machine learning tasks, where the \"best\" solution must be chosen using limited data.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "the most commonly used method of **regularization** of ill-posed problems. In statistics, the method is known as **ridge regression**, in machine learning it is known as **weight decay**, and with multiple independent discoveries.\n",
    "![](https://ds055uzetaobb.cloudfront.net/image_optimizer/37ea8b78480bb520a3e30b7113689f7b77cecc2d.png)\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "Ridge regression은 과적합을 방지하기 위해 사용됨<br>\n",
    "과적합은 분류기가 training data에 지나치게 fitting되어 실제 상황 혹은 test data에서는 좋은 성능이 나타나지 않는 상황을 말함<br>\n",
    "위 그림에서 파란색의 선은 보다 간단한 초록색의 선으로 주어진 데이터를 설명할 수 있음에도 불구하고, 훨씬 복잡한 곡선으로 fitting이 되어 있으며, 이로 인해 training data에 대한 예측정확도는 높으나 test data에 대해서는 예측정확도가 떨어짐<br>\n",
    "이와 같은 현상을 방지하기 위해서는 곡선이 지나치게 복잡해지지 않도록 parameter를 억제하면 됨<br>\n",
    "Rigde regression은 parameter에 대해 제약을 줌으로써 학습과정에서 parameter가 과도하게 변하지 않도록 함<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.974\n",
      "Test set score: 0.838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# 일반적으로 ridge regression을 쓰면 쓰지 않은 경우보다 train data에 대한 예측정확도는 떨어지고 test data는 올라가게 됨\n",
    "# 여기서는 train set에 대한 예측정확도가 같이 상승하는 진귀한 경우가 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression for feature selection\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "In statistics and machine learning, **lasso** (least absolute shrinkage and selection operator) (also Lasso or LASSO) is a regression analysis method that performs both **variable selection** and **regularization** in order to enhance the prediction accuracy and **interpretability** of the statistical model it produces. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "Lasso regression은 ridge regression과 비슷하게 parameter에 제약을 주지만, 0에 가까운 parameter를 완전히 0으로 바꿔서 결과적으로 feature를 선택하게 된다는 차이가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.804\n",
      "Test set score: 0.770\n",
      "Used features count: 78 out of 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) \n",
    "# parameter 혹은 coefficient 중에서 0이 아닌 것들의 개수를 출력\n",
    "# 2000개 중에서 78개만 선택된 것을 볼 수 있음\n",
    "# 예측률은 rigde나 일반 logistic에 비해 떨어지지만, 실제로 영향을 미치는 단어들이 어떤 것들인지 확인할 수 있다는 장점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000', '10', '100', '13', '15', '1995', '1996', '1997', '1998', '1999']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tfidf.vocabulary_)) # tfidf에 사용된 단어의 수\n",
    "tfidf_voca = tfidf.get_feature_names() # tfidf에서 단어이름을 가져옴\n",
    "tfidf_voca[:10] # 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "print((lasso_clf.coef_ != 0)[0].tolist()[:100]) \n",
    "# lasso에 사용된 단어들 중 coefficient의 사용여부를 리스트로 변환하여 앞부터 100개를 출력해 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "for i, sign in enumerate((lasso_clf.coef_ != 0)[0].tolist()):\n",
    "    if sign: selected_features.append(tfidf_voca[i]) #사용여부가 True인 단어들만 selected_features에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(selected_features) #78개의 선택된 단어들을 출력 - 즉 positive, negative에 결정적 영향을 미치는 단어들\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aliens'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_voca[(lasso_clf.coef_ != 0)[0].tolist().index(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aliens', 'also', 'and', 'any', 'as', 'attempt', 'awful', 'bad', 'batman', 'boring', 'cameron', 'carpenter', 'could', 'definitely', 'director', 'dull', 'even', 'excellent', 'family', 'great', 'hanks', 'harry', 'have', 'he', 'her', 'here', 'his', 'in', 'is', 'jackie', 'julie', 'lame', 'least', 'lebowski', 'life', 'looks', 'many', 'matrix', 'mess', 'most', 'movie', 'mulan', 'no', 'nothing', 'off', 'on', 'only', 'perfect', 'perfectly', 'plot', 'political', 'poor', 'quite', 'reason', 'ridiculous', 'scream', 'script', 'seagal', 'see', 'seen', 'shrek', 'so', 'stupid', 'supposed', 'the', 'then', 'there', 'this', 'to', 'truman', 'unfortunately', 'very', 'war', 'waste', 'well', 'west', 'why', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print([tfidf_voca[i] for i, j in enumerate((lasso_clf.coef_ != 0)[0].tolist()) if j]) #선택된 단어들을 출력하는 또다른 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension을 줄이는 또다른 방법: SVD\n",
    "lasso가 feature selection이라면, SVD는 feature extraction\n",
    "\n",
    "**Feature selection** (https://en.wikipedia.org/wiki/Feature_selection) \n",
    "\n",
    "In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n",
    "\n",
    "* simplification of models to make them easier to interpret by researchers/users,\n",
    "* shorter training times,\n",
    "* to avoid the curse of dimensionality,\n",
    "* enhanced generalization by reducing overfitting (formally, reduction of variance)\n",
    "\n",
    "Text mining에서는 단어의 수를 다양한 방법으로 줄일 수 있음\n",
    "\n",
    "* 전처리 과정에서 의미 없는 단어 삭제\n",
    "* stopwords 삭제\n",
    "* 빈도가 낮은 단어 삭제\n",
    "* 빈도가 높은 단어 삭제\n",
    "* 결과에 영향을 미치지 않는 단어 삭제\n",
    "\n",
    "**Feature extraction**: (https://en.wikipedia.org/wiki/Feature_extraction)\n",
    "\n",
    "In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.\n",
    "\n",
    "* PCA(Principal component analysis)\n",
    "* LSA(Latent semantic analysis)\n",
    "* SVD(Singular-value decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA(Principal component analysis)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "**Principal component analysis (PCA)** is a statistical procedure that uses an *orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables* called principal components. If there are {\\displaystyle n} n observations with {\\displaystyle p} p variables, then the number of distinct principal components is {\\displaystyle \\min(n-1,p)} {\\displaystyle \\min(n-1,p)}. This transformation is defined in such a way that the first principal component has the *largest possible variance* (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "**PCA(주성분분석)**: 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소\n",
    "\n",
    "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "\n",
    "![](g1.png)\n",
    "![](pca1.gif)\n",
    "![](pca2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA(Latent Semantic Analysis)\n",
    "\n",
    "**Latent semantic analysis (LSA)** is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that *words that are close in meaning will occur in similar pieces of text* (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called **singular value decomposition (SVD)** is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.\n",
    "\n",
    "**SVD(특이값 분해)** https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/\n",
    "\"*truncated SVD*는 Σ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라낸 형태. 이렇게 하면 행렬 A를 원복할 수 없게 되지만, 데이터 정보를 상당히 압축했음에도 행렬 A를 근사할 수 있음.\" **잠재의미분석(LSA)** 은 바로 이 방법을 사용\"\n",
    "\n",
    "![](tSVD.png)\n",
    "\n",
    "동영상 확인: https://en.wikipedia.org/wiki/Latent_semantic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn LSA\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "<br>\n",
    "**Dimensionality reduction using truncated SVD (aka LSA).**\n",
    "\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01525261 0.01669138 0.0133583  0.01042773 0.00741403 0.00730082\n",
      " 0.00677966 0.00583257 0.00555167 0.00520324 0.00493018 0.00489498\n",
      " 0.00458197 0.0044574  0.00432849 0.00421975 0.0041921  0.00413251\n",
      " 0.00403416 0.00398954 0.00388074 0.00379435 0.00377392 0.00370049\n",
      " 0.00361687 0.00358558 0.00350955 0.00340951 0.0033149  0.00327589\n",
      " 0.00321078 0.00319169 0.00315556 0.00311055 0.00304756 0.00302892\n",
      " 0.00300949 0.00297231 0.00293611 0.00292412 0.00290208 0.00287548\n",
      " 0.0028342  0.00280742 0.00276097 0.00274277 0.00270645 0.00268513\n",
      " 0.00267427 0.00265801 0.00258256 0.00257419 0.00256683 0.00255658\n",
      " 0.00251155 0.00250159 0.00248046 0.00247207 0.00245957 0.00245289\n",
      " 0.00241614 0.0023879  0.00236025 0.00234094 0.00233893 0.00231345\n",
      " 0.00229516 0.0022807  0.00227705 0.00224271 0.00222958 0.00222203\n",
      " 0.00220312 0.00219438 0.00218776 0.0021722  0.00215608 0.00215051\n",
      " 0.00213564 0.00212784 0.00211271 0.00208695 0.00207613 0.00206077\n",
      " 0.00204613 0.00203201 0.00200931 0.00199865 0.00199714 0.00197327\n",
      " 0.00196589 0.00194951 0.00194085 0.00193257 0.00190954 0.00190278\n",
      " 0.00187657 0.0018573  0.00185606 0.00183413]\n",
      "0.3412776971149227\n",
      "[26.28071313  3.92896916  3.51203674  3.10301032  2.61748589  2.59642773\n",
      "  2.50217867  2.32098855  2.26439245  2.19203943  2.13375967  2.12626195\n",
      "  2.0574806   2.02899959  1.99946031  1.97402069  1.96747448  1.95357853\n",
      "  1.93008885  1.91932804  1.89298024  1.87190046  1.86671996  1.84849394\n",
      "  1.82766938  1.819579    1.80027893  1.77431404  1.74951541  1.7391934\n",
      "  1.72195946  1.71670307  1.70696488  1.69474953  1.67750176  1.67234783\n",
      "  1.66697967  1.65665295  1.64652842  1.64316809  1.63696717  1.62944089\n",
      "  1.61771724  1.61004634  1.59668252  1.59144373  1.58090069  1.57474131\n",
      "  1.57139547  1.56660944  1.54422075  1.54187887  1.53957662  1.53645066\n",
      "  1.52294988  1.51981377  1.5133942   1.51083776  1.50699561  1.50498852\n",
      "  1.4936319   1.48487743  1.47627146  1.47043492  1.4695741   1.461563\n",
      "  1.45597621  1.45119181  1.4500094   1.43903939  1.43480772  1.43237878\n",
      "  1.42628258  1.42346676  1.4212888   1.41623469  1.41104021  1.40916811\n",
      "  1.4042597   1.40182779  1.39671277  1.38815761  1.38456007  1.37944892\n",
      "  1.37451998  1.36984485  1.36221159  1.35849975  1.35798477  1.34983287\n",
      "  1.34729707  1.34169375  1.33869015  1.3358442   1.32784891  1.32550655\n",
      "  1.31632998  1.30959701  1.3091189   1.30136956]\n",
      "(100, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  # train data로 학습 -> unsupervised이므로 y가 필요 없음\n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "# 현재 결과에서는 100개의 선택된 component가 전체 분산의 34% 정도를 설명하고 있음\n",
    "print(svd.singular_values_) #위 그림에서 오른쪽 가운데 대각행렬의 값\n",
    "print(svd.components_.shape) # 위 그림에서 가장 오른쪽 행렬의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n",
      "(1600, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "print(X_train_tfidf.shape) #축소하기 전의 차원\n",
    "print(X_train_svd.shape) #축소된 후의 차원, 2000 -> 100개로 줄어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA를 이용한 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.838\n",
      "Test set score: 0.790\n"
     ]
    }
   ],
   "source": [
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train) #축소된 값으로 학습\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))\n",
    "# NB, Lasso보다 좋지만 Ridge, 일반보다는 나쁜 값\n",
    "# 상황에 따라 달라질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 한글문서 분류\n",
    "다음무비(http://movie.daum.net)로부터 crawl한 영화리뷰를 이용하여 분류 연습<br>\n",
    "영화리뷰와 영화의 제목을 학습해서 주어진 리뷰내용으로 어떤 영화에 대한 리뷰인지를 예측하고자 함\n",
    "### data file 내용\n",
    "'신과함께', '코코', '라라랜드', '인피니티 워', '곤지암' 다섯개의 영화에 대해 총 1827개의 리뷰를 수집\n",
    "csv 파일 안에 리뷰내용, 평점, 영화이름 의 순으로 저장되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "text = []\n",
    "y = []\n",
    "with open('movie_data(3).csv', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        #print(row)\n",
    "        if row: #그 줄에 내용이 있는 경우에만\n",
    "            text.append(row[0]) #영화 리뷰를 text 리스트에 추가\n",
    "            y.append(row[2]) #영화이름을 text 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples: 1827\n",
      "Movie titles of reivews: {'인피니티 워', '라라랜드', '곤지암', '신과함께', '코코'}\n"
     ]
    }
   ],
   "source": [
    "print('Num of samples: {}'.format(len(text)))\n",
    "print('Movie titles of reivews: {}'.format(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, y, random_state=0)\n",
    "# 비율을 지정하지 않으면 75:25로 분할됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1370"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) #1827의 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "twitter_tag = Okt()\n",
    "#twitter_tag = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['혹시', '나', '하고', '봤는데', '역시', '나다', ';;', '편집', '과', '사운드', '로', '주는', '작은', '공포', '영화', \"'\", '푸시', \"'\", '에서', '인상', '깊게', '봤던걸', '여기', '서', '또', '보네', ';;']\n"
     ]
    }
   ],
   "source": [
    "print(twitter_tag.morphs(X_train[1])) #둘째 리뷰에 대해 형태소 단위로 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['혹시', '역시', '편집', '사운드', '공포', '영화', '푸시', '인상', '여기', '또']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tag.nouns(X_train[1]) #둘째 리뷰에서 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer(text): # Twitter 형태소 분석기의 명사추출함수를 tokenizer 함수로 사용\n",
    "    return twitter_tag.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8364963503649635\n",
      "Test score 0.6717724288840262\n",
      "(1370, 1156)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=2) #Twitter 형태소분석기에서 명사만 추출하는 함수를 tokenizer로 이용\n",
    "# twit_tokenizer 대신 twitter_tag.nouns를 직접 써도 됨\n",
    "# 하나의 문서에서만 출현한 단어는 쓸모가 없으므로 제외, 즉 최소 document frequency를 2로 설정\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train) # train data 변환 -> tfidf vector\n",
    "X_test_tfidf = tfidf.transform(X_test) # test data 변환 -> tfidf vector\n",
    "\n",
    "clf = LogisticRegression() # logistic regression 분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # 분류기 학습\n",
    "print('Train score', clf.score(X_train_tfidf, y_train)) # train data 예측정확도\n",
    "print('Test score', clf.score(X_test_tfidf, y_test)) # test data 예측정확도\n",
    "print(X_train_tfidf.shape) # 총 1156개의 명사로 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['졸잼 최고',\n",
       " '내용, 음악 , 연기력  무엇하나 빠지는것이 없네요 특히 음악은 계속 찾아 듣게되요^^',\n",
       " '아맥2D로 느즈막히 관람.... 히어로가 많이나오지만, 이걸 꽤나 잘 버무려놓음. 뻔한스토리의 틀을 벗어나려 노력한점은 높은점수를 줄만함.... 블럭버스터액션, 영상미는 말이필요없음...... 후속편 기대됨!',\n",
       " '후반부터 쫄렸다.',\n",
       " '진짜. 솔직히 한국 공포영화중에 이렇게 소재별로인건 정말 오랜만인듯; 지들끼리 소리지르고 정신없이 우왕자왕 심지어 무섭지도않어 효과음만크고 진짜최악임ㅉㅉ',\n",
       " '소문난 잔치에 먹을거 없음..ㅜㅜ',\n",
       " 'good!',\n",
       " '아 점수를 줄 수가 없네  화면은 왜그리도 흔들어 데는지........ 재미도 없고 가볍기만하고 .... 최악의 재미없는 배멀미 영화',\n",
       " '영화 보면서 펑펑물었네요~ 부모님 사랑에 대해 다시한번 생각하게 했던 영화네요^ ^',\n",
       " '슬픈 스토리지만 삶을 돌아보게 하는 영화다. 죄를 지은자는 그 벌을 고스란히 받으리라. 사회 각종범죄자들 뉘우치길 바란다.']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10] #test data에서 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인피니티 워', '라라랜드', '인피니티 워', '곤지암', '곤지암', '인피니티 워', '인피니티 워',\n",
       "       '신과함께', '코코', '신과함께'], dtype='<U6')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test_tfidf[:10]) # test data의 앞 10개에 대한 예측내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인피니티 워', '라라랜드', '인피니티 워', '곤지암', '곤지암', '인피니티 워', '인피니티 워', '곤지암', '신과함께', '신과함께']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10]) # test data 앞 10개의 실제 영화제목"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능을 개선하기 위한 노력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['혹시', '나', '하고', '봤는데', '역시', '나다', ';;', '편집', '과', '사운드', '로', '주는', '작은', '공포', '영화', \"'\", '푸시', \"'\", '에서', '인상', '깊게', '봤던걸', '여기', '서', '또', '보네', ';;']\n"
     ]
    }
   ],
   "source": [
    "# morphs()는 명사 외에도 모든 형태소를 포함\n",
    "print(twitter_tag.morphs(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8963503649635036\n",
      "Test score 0.649890590809628\n",
      "(1370, 2260)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twitter_tag.morphs, min_df=2) # 명사 대신 모든 형태소를 사용\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#명사만 사용한 것에 비해 train score는 상승, test score는 하락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('혹시', 'Noun'), ('나', 'Josa'), ('하다', 'Verb'), ('보다', 'Verb'), ('역시', 'Noun'), ('나다', 'Verb'), (';;', 'Punctuation'), ('편집', 'Noun'), ('과', 'Josa'), ('사운드', 'Noun'), ('로', 'Josa'), ('주다', 'Verb'), ('작다', 'Adjective'), ('공포', 'Noun'), ('영화', 'Noun'), (\"'\", 'Punctuation'), ('푸시', 'Noun'), (\"'\", 'Punctuation'), ('에서', 'Josa'), ('인상', 'Noun'), ('깊다', 'Adjective'), ('보다', 'Verb'), ('여기', 'Noun'), ('서', 'Josa'), ('또', 'Noun'), ('보다', 'Verb'), (';;', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter_tag.pos(X_train[1], norm=True, stem=True)) #pos()는 형태소와 품사를 함께 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer2(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "#            result.append('/'.join([word, tag]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['혹시', '하다', '보다', '역시', '나다', '편집', '사운드', '주다', '작다', '공포', '영화', '푸시', '인상', '깊다', '보다', '여기', '또', '보다']\n"
     ]
    }
   ],
   "source": [
    "print(twit_tokenizer2(X_train[1])) # 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8715328467153285\n",
      "Test score 0.6849015317286652\n",
      "(1370, 1584)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer2, min_df=2) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "# 현재까지 중에서 test score가 가장 뛰어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 형태소를 다 사용하고 품사를 알 수 있도록 하면?\n",
    "def twit_tokenizer3(text):\n",
    "    #target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        result.append('/'.join([word, tag])) #단어의 품사를 구분할 수 있도록 함\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8927007299270073\n",
      "Test score 0.6739606126914661\n",
      "(1370, 2023)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer3, min_df=2)\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#성능이 오히려 떨어지고 품사 표시 없이 전체를 다 사용한 경우에 비해 train은 떨어지고, test는 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.944\n",
      "Test set score: 0.676\n"
     ]
    }
   ],
   "source": [
    "# train score가 높으므로 ridge를 쓰면 어떨까?\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier(alpha = 1)\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# train score가 올라가는 현상이 벌어짐\n",
    "# test score가 올라갔으나 명사, 형용사, 동사를 쓴 것보다 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.718\n",
      "Test set score: 0.643\n",
      "Used features count: 240 out of 2023\n"
     ]
    }
   ],
   "source": [
    "#lasso를 쓰면?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01102934 0.01405529 0.01144551 0.0110093  0.00918016 0.00876149\n",
      " 0.00836851 0.00795922 0.00777003 0.00737423 0.00707199 0.00665546\n",
      " 0.00659729 0.00616382 0.00588976 0.00563969 0.00546929 0.00543044\n",
      " 0.00524396 0.00523751 0.0051354  0.00507564 0.00494917 0.00478661\n",
      " 0.00466253 0.0046021  0.00455272 0.00452943 0.00432636 0.0043043\n",
      " 0.00425346 0.00416261 0.00410043 0.00401296 0.00399825 0.00396182\n",
      " 0.00392779 0.00385998 0.00373325 0.00369469 0.00365484 0.00363113\n",
      " 0.00357832 0.00354544 0.00349848 0.00346928 0.00337552 0.00334918\n",
      " 0.00332458 0.00330956 0.00323571 0.00321667 0.00315217 0.00314053\n",
      " 0.00309441 0.00307477 0.0030649  0.00301359 0.00300256 0.00296081\n",
      " 0.00293836 0.00291433 0.00288938 0.00288701 0.00286437 0.00281526\n",
      " 0.00280011 0.00276482 0.00274032 0.00271676 0.00269105 0.00266254\n",
      " 0.00264105 0.00262961 0.00260318 0.00258836 0.00257263 0.0025549\n",
      " 0.00253862 0.00252722 0.00251281 0.00248844 0.00248194 0.00245121\n",
      " 0.00244755 0.00241116 0.00239864 0.0023803  0.00236743 0.00235925\n",
      " 0.00232777 0.00231998 0.00228583 0.00225879 0.00224278 0.00222963\n",
      " 0.00222807 0.00219394 0.00218403 0.00217777 0.00216885 0.00213963\n",
      " 0.00213221 0.00211726 0.0021082  0.00208491 0.00207873 0.00206984\n",
      " 0.0020442  0.00203715 0.00203109 0.00202247 0.00200684 0.00199728\n",
      " 0.00198139 0.00197578 0.00196858 0.0019567  0.00195046 0.00193928\n",
      " 0.00192188 0.00191527 0.00190398 0.00190288 0.00189315 0.00188382\n",
      " 0.00186635 0.00185401 0.00184764 0.00184161 0.0018402  0.00182065\n",
      " 0.001813   0.00179631 0.00179194 0.00178155 0.00177619 0.00177247\n",
      " 0.00176391 0.00174575 0.00174084 0.00173493 0.00172713 0.00172038\n",
      " 0.00171324 0.00169736 0.00169107 0.00168815 0.00167718 0.00166958\n",
      " 0.0016606  0.00165005 0.00164831 0.00163445 0.00162676 0.00162542\n",
      " 0.00161713 0.00161272 0.00159771 0.00159238 0.00159037 0.00158957\n",
      " 0.00156833 0.00156387 0.00155943 0.00155314 0.00154517 0.00154178\n",
      " 0.00153878 0.00153156 0.00152027 0.00151681 0.00150908 0.00150907\n",
      " 0.00150429 0.00148962 0.00148561 0.00147865 0.00147801 0.00146682\n",
      " 0.00146001 0.00145723 0.00144468 0.00143961 0.00143002 0.00142921\n",
      " 0.0014208  0.00141638 0.00141579 0.00140818 0.0014034  0.00139324\n",
      " 0.00139056 0.00138567 0.00137493 0.00137155 0.00136727 0.00135985\n",
      " 0.00135444 0.00134977 0.00134562 0.00133558 0.00133122 0.00132535\n",
      " 0.00132194 0.00131758 0.00131066 0.00130911 0.00130505 0.00129728\n",
      " 0.00129571 0.00128405 0.00127616 0.00127324 0.00126559 0.00125941\n",
      " 0.00125722 0.00125095 0.00124375 0.00124088 0.00123686 0.00123166\n",
      " 0.00122136 0.00121416 0.00120729 0.00119822 0.00119425 0.0011902\n",
      " 0.00118219 0.00117517 0.00117452 0.0011669  0.00115835 0.00114922\n",
      " 0.00114639 0.00113962 0.00113862 0.00113157 0.00112449]\n",
      "0.6274704549280449\n",
      "[7.18722998 4.37716265 3.87707769 3.80401495 3.46987465 3.38992696\n",
      " 3.31319188 3.23081401 3.19295868 3.11176267 3.0479269  2.97464527\n",
      " 2.94181282 2.84376785 2.7792728  2.72052017 2.67818072 2.66871722\n",
      " 2.62671166 2.6211839  2.59772293 2.58190277 2.54773577 2.50549056\n",
      " 2.47514125 2.46249764 2.44458218 2.44081919 2.38195771 2.37595715\n",
      " 2.36213857 2.33646299 2.31897222 2.29462272 2.28988032 2.27951064\n",
      " 2.2696032  2.25224959 2.21275983 2.20157946 2.18941753 2.18229454\n",
      " 2.16700863 2.15629757 2.14384191 2.13417342 2.10461423 2.09580568\n",
      " 2.08809633 2.08457671 2.06118326 2.05524985 2.03983521 2.02947381\n",
      " 2.0156339  2.00808902 2.00484508 1.98892489 1.9848698  1.97080544\n",
      " 1.96433732 1.95685115 1.94851579 1.94618356 1.93878407 1.92354149\n",
      " 1.91631125 1.90417728 1.8957369  1.88786307 1.8786183  1.86911358\n",
      " 1.86110158 1.85717919 1.84914318 1.84466386 1.83680751 1.83047136\n",
      " 1.8257132  1.82052779 1.81602147 1.80661829 1.80431053 1.79331989\n",
      " 1.79183982 1.77843174 1.77363887 1.76688172 1.76225481 1.75901716\n",
      " 1.7472136  1.74547341 1.73138953 1.72112568 1.71500666 1.71000327\n",
      " 1.70948057 1.69627556 1.69249592 1.69011496 1.68727252 1.67602962\n",
      " 1.67226219 1.66647038 1.66276613 1.65396201 1.65116958 1.64756305\n",
      " 1.63885047 1.6351727  1.63216299 1.62859852 1.62272231 1.6199386\n",
      " 1.61197651 1.60973638 1.60686067 1.6019874  1.59937235 1.59485625\n",
      " 1.58767203 1.58485923 1.58104529 1.57976164 1.57568875 1.57192692\n",
      " 1.56637507 1.55945183 1.55681473 1.55449401 1.55349754 1.54521608\n",
      " 1.54228311 1.53494218 1.53323466 1.52873209 1.52685008 1.52467317\n",
      " 1.52095301 1.51360868 1.51136655 1.50844406 1.50501843 1.50295063\n",
      " 1.4990343  1.49203042 1.48921439 1.48844592 1.48323386 1.47971939\n",
      " 1.47588973 1.47125412 1.47030967 1.46441287 1.46089164 1.4600098\n",
      " 1.45645421 1.45430045 1.44757356 1.4451948  1.44418574 1.44381833\n",
      " 1.43474477 1.43241517 1.43075667 1.42734688 1.4239681  1.42213184\n",
      " 1.42056782 1.41722678 1.4129636  1.41085354 1.40692287 1.40682537\n",
      " 1.40490821 1.39769447 1.39580527 1.39275219 1.3922484  1.38695654\n",
      " 1.38382998 1.38275183 1.37645068 1.37404564 1.36964869 1.36905779\n",
      " 1.36514341 1.36291003 1.36262579 1.35901456 1.35666126 1.35220907\n",
      " 1.35071654 1.34873489 1.34301621 1.34134439 1.33943739 1.33542613\n",
      " 1.33276808 1.33056349 1.3284238  1.32349714 1.32147413 1.31838175\n",
      " 1.31677435 1.31457724 1.31126584 1.31041624 1.30838878 1.30434815\n",
      " 1.30356031 1.29770436 1.29424264 1.29220091 1.28853219 1.28520775\n",
      " 1.2841347  1.28089417 1.27725376 1.27569374 1.27362989 1.27093976\n",
      " 1.26565419 1.26203966 1.25829658 1.25355624 1.25168127 1.24946418\n",
      " 1.24542796 1.24144044 1.24113263 1.23706028 1.2325742  1.22773662\n",
      " 1.22629484 1.22251701 1.22209418 1.21818685 1.21495199]\n",
      "(239, 2023)\n"
     ]
    }
   ],
   "source": [
    "#lsa를 쓰면?\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=239, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  \n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "print(svd.singular_values_) \n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.771\n",
      "Test set score: 0.654\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train)\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soo hyun\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1370, 1584)\n",
      "Test set dimension: (457, 1584)\n",
      "Train set score: 0.801\n",
      "Test set score: 0.685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer=twit_tokenizer2, min_df=2).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train)))\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 엔그램 \n",
    "## 20 Newsgroups data 준비\n",
    "\n",
    "http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "<br><br>\n",
    "특징: \n",
    "* train set과 test set이 별도로 분리되어 있음.\n",
    "* categories 매개변수를 이용하여 20개의 topic 중에서 원하는 topic을 선택할 수 있음\n",
    "* remove를 이용하여 필요없는 부분을 삭제할 수 있음\n",
    "* 각 set 내에서 .data 는 text 내용을, .target은 숫자로 변경된 label(category)를 가져오는 데 사용됨\n",
    "\n",
    "따라서 20 newsgroups는 train_test_split을 이용하여 train set과 test set을 분리할 필요가 없음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 2034\n",
      "test set size: 1353\n",
      "selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "train labels: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "print('train set size:', len(newsgroups_train.data))\n",
    "print('test set size:', len(newsgroups_test.data))\n",
    "print('selected categories:', newsgroups_train.target_names)\n",
    "print('train labels:', set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Train set text samples: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "##Train set label smaples: 1\n",
      "##Test set text samples: TRry the SKywatch project in  Arizona.\n",
      "##Test set label smaples: 2\n"
     ]
    }
   ],
   "source": [
    "print('##Train set text samples:', newsgroups_train.data[0])\n",
    "print('##Train set label smaples:', newsgroups_train.target[0])\n",
    "print('##Test set text samples:', newsgroups_test.data[0])\n",
    "print('##Test set label smaples:', newsgroups_test.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newsgroups_train과 newsgroups_test로부터 .data와 .target을 이용하여 X_train, X_test, y_train, y_test을 추출하여 text document classification을 수행하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\soo\n",
      "[nltk_data]     hyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 11483)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'), \n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.966\n",
      "Test set score: 0.761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26550)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range=(1, 2),\n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'cause can't\", \"'em better\", \"'expected errors'\", \"'karla' next\", \"'nodis' password\", \"'official doctrine\", \"'ok see\", \"'sci astro'\", \"'what's moonbase\", 'aas american']\n"
     ]
    }
   ],
   "source": [
    "bigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 1]\n",
    "print(bigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.969\n",
      "Test set score: 0.756\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 32943)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
    "                        decode_error ='ignore', \n",
    "                        lowercase=True, \n",
    "                        stop_words = stopwords.words('english'),\n",
    "                        ngram_range=(1, 3),\n",
    "                        max_df=0.5,\n",
    "                        min_df=2).fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'em better shots\", \"'expected errors' basically\", \"'karla' next one\", \"'nodis' password also\", \"'official doctrine think\", \"'ok see warning\", \"'what's moonbase good\", 'aas american astronautical', 'ability means infallible', 'able accept donations']\n"
     ]
    }
   ],
   "source": [
    "trigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 2]\n",
    "print(trigram_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.969\n",
      "Test set score: 0.758\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression() #분류기 선언\n",
    "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.976\n",
      "Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
    "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.761\n",
      "Test set score: 0.695\n",
      "Used features count: 247 out of 32943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
    "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.974\n",
      "Test set score: 0.758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto', kernel='linear')\n",
    "clf.fit(X_train_tfidf, y_train) \n",
    "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
    "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
